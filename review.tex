\title{Notes}
\author{Ricson}
\date{\today}
\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[expansion=false]{microtype}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{algpseudocode}

% let the macro madness begin

\newcommand{\nec}[1]{\newcommand{#1}}
\newcommand{\rec}[1]{\renewcommand{#1}}

%environments 
\nec{\eq}[1]{\begin{align*}#1\end{align*}}
\nec{\enum}[1]{\begin{enumerate}#1\end{enumerate}}
\nec{\itmz}[1]{\begin{itemize}#1\end{itemize}}
\nec{\alg}[1]{\begin{algorithmic}#1\end{algorithmic}}

%symbols
\nec{\ra}{\rightarrow}
\nec{\la}{\leftarrow}

\nec{\x}{\mathbf{x}}
\nec{\y}{\mathbf{y}}
\rec{\o}{\mathbf{o}}
\nec{\RR}{\mathbb{R}}

\nec{\haf}{\frac{1}{2}}
\nec{\nhaf}{-\haf}
\nec{\nmz}{\frac{1}{Z}}

\nec{\const}{\text{const}}

%calculus
\nec{\grad}{\nabla}
\nec{\jac}{\mathbf{J}}
\nec{\hess}{\mathbf{H}}
\nec{\parf}[2]{\frac{\partial #1}{\partial #2}}
\nec{\parfsq}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}

%linear algebra
\nec{\qf}[2]{#1^T #2 #1}
\nec{\qfo}[3]{\qf{(#1-#3)}{#2}}

%probability
\nec{\uvnml}[3]{\frac{1}{\sqrt{2 \pi #3^2}} \exp{\left(-\frac{(#1-#2)^2}{2 #3^2}\right)}}
\nec{\uvnmlp}[3]{\frac{\lambda}{\sqrt{2 \pi}} \exp{\left(-\frac{#3(#1-#2)^2}{2}\right)}}
\nec{\mvnml}[3]{(2 \pi)^{-\frac{D}{2}} |#3|^{\nhaf} \exp{\left( \nhaf \qfo{#1}{#3^{-1}}{#2} \right)}}
\nec{\mvnmlp}[3]{(2 \pi)^{-\frac{D}{2}} |#3|^{\haf} \exp{\left( \nhaf \qfo{#1}{#3}{#2} \right)}}

%message passing stuff
\nec{\msg}[2]{\mu (#1 \rightarrow #2)}
\rec{\ne}[2]{N(#1) \setminus #2}

\begin{document}
\maketitle

\tableofcontents

\newpage


\section{Probability Theory}

\subsection{Conjugate Priors}

A prior distribution is conjugate to a conditional distribution (also known as the likelihood), when it makes Bayes' Theorem work out nicely.

\eq{
  p(x|y) \propto p(y|x)p(x)
}

When applying Bayes' Theorem, we want $p(x)$ and $p(x|y)$ to come from the same family of distributions, so that we can apply this update procedure as many times as we want. This requires $p(x)$ to be a conjugate prior to $p(y|x)$.

For example, let's consider the multinomial distribution parameterized by $\vec p$.

\eq{
  p(\y|\x) &= \binom{n}{y_1, y_2, \ldots y_k} \prod_i x_i^{y_i}
}

The conjugate prior to this distribution is the dirichlet distribution, which is paramerized by $\vec \alpha$

\eq{
  p(\x) &= \frac{1}{Z} \prod_i x_i^{\alpha_i-1}
}

Now let's see what happens when we multiply them together

\eq{
  p(\x|\y) &= \frac{1}{Z} \prod_i x_i^{\alpha_i-1} x_i^{y_i} \\
  &= \frac{1}{Z} \prod_i x_i^{\alpha_i+y_i-1}
}

So we've basically updated $\vec \alpha' \leftarrow \vec \alpha+\y$.

Let's attempt to find the conjugate distribution for the normal distribution in the case of known covariance.

\eq{
  P(y|x) &= \uvnml{y}{x}{\sigma} 
}

Ideally, we want a distribution for $x$ so that when we multiply by this term, we end up with a distribution in the same family. It is easy to see that we should ignore the normalization term, and focus on the quadratic term in the exponent. Let's try a gaussian in $x$.

\eq{
  p(x) &= \uvnml{x}{\mu}{\sigma_0} \\
  p(x|y) &= p(y|x)p(x) \\
  &= \uvnml{y}{x}{\sigma}  \uvnml{x}{\mu}{\sigma_0} \\
  &= \nmz \exp\left(\nhaf \left( \frac{(y-x)^2}{\sigma^2} + \frac{(x-\mu)^2}{\sigma_0^2} \right)\right)
}

We can simply the guts of this expression. At this point, notice that we only need the quadratic and linear terms of $x$. 

\eq{
  \frac{(y-x)^2}{\sigma^2} + \frac{(x-\mu)^2}{\sigma_0^2} &= \frac{\sigma_0^2 (y-x)^2 + \sigma^2 (x-\mu)^2 }{\sigma^2 \sigma_0^2} \\
  &= \frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2}x^2 - \frac{2(\sigma^2 \mu + \sigma_0^2 y)}{\sigma^2 \sigma_0^2} x + \const \\
  &= \frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2}
  \left( x^2 - \frac{\sigma^2 \sigma_0^2}{\sigma_0^2 + \sigma^2} \frac{2(\sigma^2 \mu + \sigma_0^2 y)}{\sigma^2 \sigma_0^2}x + \const \right) \\
  &= \frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2}
  \left( x^2 - 2\frac{\sigma^2 \mu + \sigma_0^2 y}{\sigma_0^2 + \sigma^2} x + \const \right) \\
}

Therefore $p(x)$ is a gaussian with precision $\frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2}$ and mean $\frac{\sigma^2 \mu + \sigma_0^2 y}{\sigma_0^2 + \sigma^2}$. Notice that the mean is a weighted average between the prior and the likelihood.

% what happens when the covariance is unknown?

% multi sample case

% multi dimensional case

\subsection{Manipulations of Multivariate Gaussians}

\subsection{Extremal Value Distributions}

\section{Information Theory}

\subsection{Basics}

\subsection{KL Divergence}

\subsection{Fisher Information Matrix}

\section{Linear Algebra}

\subsection{Identities}

\subsection{Singular Value Decomposition}

\subsection{Principle Component Analysis}

\section{Probabilistic Graphical Models}

\subsection{Forward Backward Algorithm}

The key to deriving the forward backward algorithm for the linear-chain CRF is to realize that there is no fundamental difference between HMM and CRF.
The likelihood of a linear-chain CRF is defined to be the product of all the potentials. But before we get mixed up in math here: $o_i$ denotes observation and $x_i$ denotes the hidden state.

\eq{
P(\x) = \prod_i \phi(x_i) \prod_{i,i+1} \phi(x_i, x_{i+1})
}

On the other hand, the likelihood of the equivalent HMM model is the following:

\eq{
P(x) = \prod_i P(o_i | x_i) P(x_i | x_{i+1})
}
Note this product is incorrect when $i = n$, but let's ignore that.

Now if you squint really closely, you're realize that these two equations are actually the same thing. We can just replace $P(o_i|x_i)$ with $\phi(x_i)$ and $P(x_i|x_{i+1})$ with $\phi(x_i, x_{i+1})$. So it suffices to solve inference on the general setting of CRFs. However, we'll start on HMMs simply to avoid a bit of headaches.

We want to solve for $P(x_k|\o)$. The key is the following manipulation:

\eq{
  P(x_k|\o) &= \frac{P(\o|x_k)p(x_k)}{P(\o)} \\
  &\propto P(\o|x_k)p(x_k) \\
  &= P(\o_{1:k}|x_k)P(\o_{k+1:n}|x_k)P(x_k) \\
}

Let's take that first term and apply Bayes' Theorem again

\eq{
  P(\o_{1:k}|x_k) &= \frac{P(x_k | \o_{1:k}) P(\o_{1:k})}{P(x_k)} \\
  &\propto \frac{P(x_k | \o_{1:k})}{P(x_k)}
}

And substitute it back in...

\eq{
  P(\o_{1:k}|x_k)P(\o_{k+1:n}|x_k)P(x_k) &= \frac{P(x_k|\o_{1:k})}{P(x_k)}P(\o_{k+1:n}|x_k)P(x_k) \\
  &= P(x_k|\o_{1:k})P(\o_{k+1:n}|x_k)
}

So now we just need to figure out how to compute $P(x_k|\o_{1:k})$ and $P(o_{k+1:n}|x_k)$. We can do this inductively. To do this, we specify that each $x_i$ is a categorical variables in one of $m$ classes indexed by $j$. In addition, we need $\pi$, the initial distribution of $x_1$ and $T$ being $P(x_{k+1|k})$ and also $Q$ being $P(o_i|x_k)$. 

Our base case $P(x_1|o_1)$ can be solved with Bayes' Theorem

\eq{
  P(x_1|o_1) &= \frac{P(o_1|x_1)P(x_1)}{P(x_1)} \\
  &\propto P(o_1|x_1)P(x_1) \\
  &= Q\pi
}

Then we can simply normalize $Q\pi$ to be a probability vector. Now we will show the inductive step.

\eq{
  P(x_k|\o_{1:k}) &= P(x_k|o_k,\o_{1:k-1}) \\
  &= \frac{P(o_k|x_k,\o_{1:k-1})P(x_k|\o_{1:k-1})}{P(o_k|\o_{1:k-1})} \\
  &\propto P(o_k|x_k,\o_{1:k-1})P(x_k|\o_{1:k-1}) \\
  &= P(o_k|x_k)P(x_k|\o_{1:k-1}) \\
  &= P(o_k|x_k) \sum_j P(x_k|x_{k-1} = j)P(x_{k-1}=j|\o_{1:k-1}) \\
}

All of these quantities are clearly obtainable. We must simply normalize over values of $x_k$ in order to get a proper probability distribution.

In the backward case, we start off with the base case $P(o_n|x_{n-1})$

\eq{
  P(o_n|x_{n-1}) &= \sum_j P(o_n|x_n = j)P(x_n=j|x_{n-1})
}

And for the inductive case we have

\eq{
  P(o_{k+1:n}|x_k) &= \sum_j P(o_{k+1:n}|x_{k+1}=j)P(x_{k+1}=j|x_k) \\
  &= \sum_j P(\o_{k+2:n}|x_{k+1}=j)P(o_{k+1}|x_{k+1} = j)P(x_{k+1}=j|x_k) \\
}

Again, all of these quantities are computable, one via induction, so we are finished here. For CRFs, since factors are not necessarily probabilities, we can carry out an extra normalization step at the end.
\subsection{Message Passing}

Some notation:
\itmz{
  \item $x$ is the variable node for which we want the marginal distribution
  \item $\x$ is the total set of variables in the graphic
  \item $f_s$ refers to a factor which is a neighbor of $x$
  \item $x_i'$ refers to the neighbors of $f_s$ excluding $x$.
  \item $f'_k$ refers to a factor which is a neighbor of $x'$
  \item $X_s$ refers to the set of variables $x$ which are below $f_s$
  \item $X_j$ refers to the set of variables $x$ which are below $x_j$ exclusive.
  \item $\msg{s}{x}$ refers to a message from $s$ to $x$.
}

Furthermore, use $F_s(x_j, X_s)$ to denote the product of the factors in the subgraph rooted at $f_s$. Similarly, $G_j(x_j, X_j)$ is used to denote the product of factors in the subgraph rooted at $x_j$. 

We want to solve for the distribution $p(x)$.

\eq{
  p(x) &= \sum_{\x \setminus x} p(\x) \\
  &= \prod_{s \in N(x)} \sum_{X_s} F_s(x, X_s) \\
}

Now note that $\sum_{X_s} F_s(x, X_s)$ is a quantity each factor node $s$ could compute by itself, so it is a good candidate for being a message. Let's say that

\eq{
  \msg{s}{x} &= \sum_{X_s} F_s(x, X_s)
  \intertext{or more generally that}
  \msg{s}{x_i} &= \sum_{X_s} F_s(x_i, X_s)
}

Then we can substitute this back into the equation fork
\eq{
  p(x) &= \prod_{s \in N(x)} \msg{x}{s}
}

It remainds to derive an algorithm for computing $\sum_{X_s} F_s(x, X_s)$ efficiently. First let us derive an expansion for $F_s$.

\eq{
  F_s(x, X_s) &= f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} G_j(x_j, X_j) \\
}

Let $X_s'$ denote $X_s \setminus N(s)$. 

\eq{
  \msg{s}{x} &= \sum_{X_s} f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} G_j(x_j, X_j) \\
  &= \sum_{\ne{s}{x}} \sum_{X_s'} f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} G_j(x_j, X_j) \\
  &= \sum_{\ne{s}{x}} f_s(x, \ne{s}{x}) \sum_{X_s'} \prod_{j \in \ne{s}{x}} G_j(x_j, X_j) \\
  &= \sum_{\ne{s}{x}} f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} \sum_{X_j} G_j(x_j, X_j) \\
}

Just like we noticed that the sum of $F_s$ might be a good candidate for being a message, the sum of $G_j$ is a good candidate, because it can be computed by variable $j$ with no knowledge of anything else.

Let $\msg{j}{s} = \sum_{X_j} G_j(x_j, X_j)$. Then our equation from beforehand becomes

\eq{
  \msg{s}{x} &= \sum_{\ne{s}{x}} f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} \msg{j}{s} \\
}

So in order for factor $s$ to send variable $x$ the message, it must collect the messages from it's neighbors excepting $x$, take the product, and then marginalize over all variables except $x$.

But what are the messages from it's neighbors? How do we compute $G_j(x_j, X_j)$? Notice that $G_j$ is simply the product of all factors below it, which is the product of the factors in each subtree.

\eq{
  G_j(x_j, X_j) &= \prod_k F_k(x_j, X_k)
}

So now we can compute our message: 

\eq{
  \msg{j}{s} &= \sum_{X_j} G_j(x_j, X_j) \\
  &= \sum_{X_j} \prod_{k \in \ne{j}{s}} F_k(x_j, X_k) \\
  &= \prod_{k \in \ne{j}{s}} \sum_{X_k} F_k(x_j, X_k) \\
  &= \prod_{k \in \ne{j}{s}} \msg{k}{j}
}

So in order for a variable $x_j$ to send factor $f_s$ a message, it must collect the messages from it's child factors, then take the product of them all. Now the whole algorithm is clear. Starting from the leaves, we propagate messages upwards according to these rules until we reach node $x$. At $x$, we simply take the product of all incoming messages and read off the marginal probabilities.

There is just one issue -- the matter of the base cases. When the leaf is a variable, we can see that the product of the incoming messages (there are none) is always 1. Then according to the formula, the value is simply $f(x_l)$.

When the leaf is a factor, the outgoing message is simply the product of all incoming messages, which is always 1. Then the message is also 1.

Another detail is whether there is a more efficient strategy to computing the marginal probabilities of all the nodes (not just one) instead of having to repeat this process once for every single node in the graph, which adds on a factor $n$ complexity.

All that is required to compute the marginal of a given node is that the surrounding factors have sent their message to that factor. Therefore, we can simply run the algorithm above to find the marginal for the root, and then have every variable and factor send messages downwards, starting from the root. This enables every variable node to compute its marginal.

Another small detail is that we actually send $k$ messages at a time, one for each value of each variable $x_i$. 

How can we generalize this algorithm to general graphs, possibly with cycles? Well it turns out that if we initialize all the messages to 1, and then iteratively run this algorithm to simultaneously update all messages, you eventually get the right answer. Maybe. 

\section{Linear and Nonlinear Bayesian Models}

\subsection{Bayesian Linear Regression}

\subsubsection{Evidence Approximation}

\subsection{Bayesian Logistic Regression}

\subsubsection{Laplace Approximation}

\subsection{Spline and Wavelet Bases}

\subsection{Bayesian Neural Networks}
\section{Approximate Inference}

\subsection{Expectation Maximization}

\subsection{Variational Inference}

\subsubsection{Efficient Inference in Fully Connected Gaussian CRFS}

A very popular class of CRFs are dense gaussian crfs, used for image processing. In this case, variables are arranged in an n by n grid, with given unary potentials and pairwise potentials between every pair of points in the grid. 

\section{Sampling Algorithms}

\subsection{Importance Sampling}

\subsection{Monte Carlo Markov Chain}

\subsubsection{Metropolis Hastings}

\subsubsection{Reversible Jump MCMC}

\subsection{Gibbs Sampling}

\section{Optimization}

\subsection{Lagrange Multipliers}

\subsection{Lagrangian Duality}

\subsection{KKT Conditions}

\subsection{SGD Variants}

\subsection{BADMM Algorithm}

\section{Reinforcement Learning}

\subsection{Optimal Control}

\subsubsection{LQR}

\subsubsection{MPC and ILQR}

\subsubsection{Stochastic Optimal Control}

\subsection{Policy Gradients}

\subsection{Natural Policy Gradients}

\subsection{Trust Region Policy Optimization}

\subsection{Guided Policy Search}

\subsection{End to End Deep Visuomotor Policies}

\section{Neural Networks}

\subsection{Variational Autoencoders}

\subsection{Architectures}

\subsubsection{Neural Turing Machine}

\subsubsection{Neural GPU}

\subsubsection{Grid LSTM}

\section{Miscellaneous}

\subsection{Matrix Differentials}

First let's define a few functions to play around with.

\itmz{
\item $f : \RR^n \ra \RR$
\item $g : \RR^n \ra \RR^m$
}

Now for some definitions, using what I believe is known as the numerator notation for matrix calculus (as opposed to denominator).

\itmz{
\item $\grad f$ is a column-vector in $\RR^n$ 
\item $\jac f$ is a row-vector in $\RR^n$
\item $\hess f$ is a matrix of size $n$ by $n$
\item $\jac g$ is a matrix of size $m$ by $n$
}

\itmz{
\item $[\grad f]_i$ is $\parf{f}{x_i}$
\item $\jac f$ is $(\grad f)^T$
\item $[\hess f]_{ij}$ is $\parfsq{f}{x_i}{x_j}$
\item $[\jac g]_{ij}$ is $\parf{f_i}{x_j}$
}

In order to differentiate expressions, we will compute differentials instead. How do we go from computing differentials to differentiating?

If $d(f(X)) = AdX$ then $\parf{f}{X} = A$. Actually we can get a bit more lax than that. 





\subsection{Calculus of Variations}

\subsection{Echo State Networks}

\subsection{Gumbel Trick}

\subsection{Support Vector Machines}

\subsection{Gaussian Processes}

\subsection{Independent Component Analysis}

In ICA, we have some random variables $s$ and some square mixing matrix $M$, and the observed variables are $x = Ms$. We want to find $x$. In order to do this, we assume $s$ is drawn from the logistic distribution, which is the distribution with the cumulative distribution function $\sigma(x)$. Furthermore, we assume each of the source variables is independent. We want to learn the unmixing matrix $U = M^{-1}$. 

Let's try to solve this with maximum likelihood.

\eq{
  p(s) &= p(Ux) \left| U \right| \\
  &= |U| \prod_{u \in U} \sigma'(u^Tx) \\
  \log p(s) &= \log |U| + \sum_{u \in U} \log \sigma'(u^T x)
}

Now at this point you just run gradient descent to get the answer. It is surprisingly easy.

Another ICA algorithm involves kurtosis, which is a measure of nongaussianity. Kurtosis is the expected value of $x^4$. One may ask why we do not use skewness, which is the expectation of $x^3$. I'm not sure myself, although I have read vague and unsubstantiated claims to the effect that kurtosis somehow encompasses skewness through something called a one-bit-matching theorem.

The algorithm performs kurtosis maximization in order to find the directions along which the data is projected into the least gaussian shape possible, but that's all I know about it.

\end{document}