\title{Study Notes}
\author{Ricson Cheng}
\date{\today}
\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[expansion=false]{microtype}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{color}
\usepackage{soul}

\allowdisplaybreaks

% let the macro madness begin

\newcommand{\nec}[1]{\newcommand{#1}}
\newcommand{\rec}[1]{\renewcommand{#1}}

%environments 
\nec{\eq}[1]{\begin{align*}#1\end{align*}}
\nec{\enum}[1]{\begin{enumerate}#1\end{enumerate}}
\nec{\itmz}[1]{\begin{itemize}#1\end{itemize}}
\nec{\alg}[1]{\begin{algorithmic}#1\end{algorithmic}}
\nec{\paren}[1]{\left( #1 \right)}
\nec{\sparen}[1]{\left[ #1 \right]}
\nec{\bexp}[1]{\exp \paren{#1}}
\nec{\nexp}[1]{\bexp{-\paren{#1}}}
\nec{\nhexp}[1]{\bexp{\nhaf #1}}

%symbols
\nec{\ra}{\rightarrow}
\nec{\la}{\leftarrow}

\nec{\x}{\mathbf{x}}
\nec{\y}{\mathbf{y}}
\rec{\o}{\mathbf{o}}
\rec{\u}{\mathbf{u}}
\nec{\RR}{\mathbb{R}}
\nec{\bfu}{\mathbf{u}}

\nec{\haf}{\frac{1}{2}}
\nec{\nhaf}{-\haf}
\nec{\nmz}{\frac{1}{Z}}

\nec{\const}{\text{const}}
\nec{\smx}{\text{softmax}}
\nec{\samp}{\text{sample}}
\nec{\argmax}{\text{argmax}}

\nec{\inv}[1]{\frac{1}{#1}}

%calculus
\nec{\grad}{\nabla}
\nec{\gradt}{\nabla_\theta}
\nec{\jac}{\mathbf{J}}
\nec{\hess}{\mathbf{H}}
\nec{\parf}[2]{\frac{\partial #1}{\partial #2}}
\nec{\parfsq}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\rec{\st}{\text{subject to }}
\nec{\lag}{\mathcal{L}}

%linear algebra
\nec{\qf}[2]{#1^T #2 #1}
\nec{\qfo}[3]{\qf{(#1-#3)}{#2}}
\nec{\minv}[1]{#1^{-1}}
\nec{\mdet}[1]{\left| #1 \right|}
\nec{\tr}{\text{tr}}
\nec{\sos}[1]{#1^T #1}
\nec{\scov}[1]{#1 #1^T}

\nec{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\nec{\mat}[1]{\begin{matrix} #1 \end{matrix}}
\nec{\vt}[2]{\begin{bmatrix} #1 \\ #2 \end{bmatrix}}
\nec{\mt}[4]{\begin{bmatrix} #1 & #2 \\ #3 & #4 \end{bmatrix}}
\nec{\vtx}{\vt{x_a}{x_b}}
\nec{\vtmu}{\vt{\mu_a}{\mu_b}}
\nec{\mtlambda}{\mt{\Lambda_{aa}}{\Lambda_{ab}}{\Lambda_{ba}}{\Lambda_{bb}}}

%probability
\nec{\uvnml}[3]{\frac{1}{\sqrt{2 \pi #3^2}} \exp{\left(-\frac{(#1-#2)^2}{2 #3^2}\right)}}
\nec{\uvnmlp}[3]{\sqrt{\frac{#3}{2\pi}} \exp{\left(-\frac{#3(#1-#2)^2}{2}\right)}}
\nec{\mvnml}[3]{(2 \pi)^{-\frac{D}{2}} |#3|^{\nhaf} \exp{\left( \nhaf \qfo{#1}{#3^{-1}}{#2} \right)}}
\nec{\mvnmlp}[3]{(2 \pi)^{-\frac{D}{2}} |#3|^{\haf} \exp{\left( \nhaf \qfo{#1}{#3}{#2} \right)}}
\nec{\nml}[2]{\mathcal{N}(#1, #2)}
\nec{\kl}[2]{\mathcal{D}_\text{KL}(#1||#2)}
\nec{\cov}{\text{Cov}}

\nec{\gammadist}[3]{\frac{#3^{#2}}{\Gamma(#2)} #1^{#2-1} e^{-#3 #1}}
\nec{\gam}[2]{\mathbf{\Gamma}(#1, #2)}
\nec{\avgsum}{\frac{1}{n}\sum}

%message passing stuff
\nec{\msg}[2]{\mu (#1 \rightarrow #2)}
\rec{\ne}[2]{N(#1) \setminus #2}

% policies
\nec{\pol}{\pi_\theta}

% convenience
\nec{\todo}[1]{{\color{red} \textbf{ \hl{todo: #1}}}}

%optimal control
\nec{\Cuu}{C_{uu}}
\nec{\Cxx}{C_{xx}}
\nec{\Cux}{C_{ux}}
\nec{\Cxu}{C_{xu}}
\nec{\cxt}{c_x^T}
\nec{\cut}{c_u^T}
\nec{\Dum}{D_u}
\nec{\Dxm}{D_x}
\nec{\vecd}{\vec d}
\nec{\vecp}{\vec p}
\nec{\vecdt}{\vec{d\,}^T}
\nec{\vecpt}{\vec{p\,}^T}
\nec{\vecq}{\vec q}
\nec{\veck}{\vec k}
\nec{\vecm}{\vec m}

%for rotation matrix convenience
\nec{\calpha}{\cos(\alpha)}
\nec{\salpha}{\sin(\alpha)}
\nec{\cbeta}{\cos(\beta)}
\nec{\sbeta}{\sin(\beta)}
\nec{\cgamma}{\cos(\gamma)}
\nec{\sgamma}{\sin(\gamma)}

\begin{document}
\maketitle

These are a collection of personal notes, and are not intended to teach or introduce material.

\tableofcontents

\newpage

\paragraph{Todo}

\itmz{
\item natural PG
\item architectures
\item approximate inference
}
  
\section{Probability Theory}

\subsection{Conjugate Priors}

A prior distribution is conjugate to a conditional distribution (also known as the likelihood), when it makes Bayes' Theorem work out nicely.

\eq{
  p(x|y) \propto p(y|x)p(x)
}

When applying Bayes' Theorem, we want $p(x)$ and $p(x|y)$ to come from the same family of distributions, so that we can apply this update procedure as many times as we want. This requires $p(x)$ to be a conjugate prior to $p(y|x)$.

For example, let's consider the multinomial distribution parameterized by $\vec p$.

\eq{
  p(\y|\x) &= \binom{n}{y_1, y_2, \ldots y_k} \prod_i x_i^{y_i}
}

The conjugate prior to this distribution is the dirichlet distribution, which is paramerized by $\vec \alpha$

\eq{
  p(\x) &= \frac{1}{Z} \prod_i x_i^{\alpha_i-1}
}

Now let's see what happens when we multiply them together

\eq{
  p(\x|\y) &= \frac{1}{Z} \prod_i x_i^{\alpha_i-1} x_i^{y_i} \\
  &= \frac{1}{Z} \prod_i x_i^{\alpha_i+y_i-1}
}

So we've basically updated $\vec \alpha' \leftarrow \vec \alpha+\y$.

Let's attempt to find the conjugate distribution for the normal distribution in the case of known covariance.

\eq{
  P(y|x) &= \uvnml{y}{x}{\sigma} 
}

Ideally, we want a distribution for $x$ so that when we multiply by this term, we end up with a distribution in the same family. It is easy to see that we should ignore the normalization term, and focus on the quadratic term in the exponent. Let's try a gaussian in $x$.

\eq{
  p(x) &= \uvnml{x}{\mu}{\sigma_0} \\
  p(x|y) &= p(y|x)p(x) \\
  &= \uvnml{y}{x}{\sigma}  \uvnml{x}{\mu}{\sigma_0} \\
  &= \nmz \exp\left(\nhaf \left( \frac{(y-x)^2}{\sigma^2} + \frac{(x-\mu)^2}{\sigma_0^2} \right)\right)
}

We can simply the guts of this expression. At this point, notice that we only need the quadratic and linear terms of $x$. 

\eq{
  \frac{(y-x)^2}{\sigma^2} + \frac{(x-\mu)^2}{\sigma_0^2} &= \frac{\sigma_0^2 (y-x)^2 + \sigma^2 (x-\mu)^2 }{\sigma^2 \sigma_0^2} \\
  &= \frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2}x^2 - \frac{2(\sigma^2 \mu + \sigma_0^2 y)}{\sigma^2 \sigma_0^2} x + \const \\
  &= \frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2}
  \left( x^2 - \frac{\sigma^2 \sigma_0^2}{\sigma_0^2 + \sigma^2} \frac{2(\sigma^2 \mu + \sigma_0^2 y)}{\sigma^2 \sigma_0^2}x + \const \right) \\
  &= \frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2}
  \left( x^2 - 2\frac{\sigma^2 \mu + \sigma_0^2 y}{\sigma_0^2 + \sigma^2} x + \const \right) \\
}

Therefore $p(x)$ is a gaussian with precision $\frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2}$ and mean $\frac{\sigma^2 \mu + \sigma_0^2 y}{\sigma_0^2 + \sigma^2}$. Notice that the mean is a weighted average between the prior and the likelihood.

In the above case, we assumed that the covariance was fixed and known. What happens when it is unknown? Our distribution becomes something like

\eq{
  P(y|x,\sigma) &= \uvnml{y}{x}{\sigma} 
}

Meanwhile, we are looking for a joint conjugate prior $P(x, \sigma)$. What if we let $x$ be normal like before, and also make $\sigma^2$ a normal random variable? Then

\eq{
  p(x,\sigma|y) &\propto p(y|x,\sigma) p(x,\sigma) \\
  &= \uvnml{y}{x}{\sigma} \cdot \uvnml{x}{\mu}{\sigma_0} \\
  &\qquad \cdot \uvnml{\sigma}{\mu_\sigma}{\sigma_\sigma} \\
  &= \nmz \exp \left(-\left(\frac{(y-x)^2}{2\sigma^2} + \frac{(x-\mu)^2}{2\sigma_0^2} + \frac{(\sigma-\mu_\sigma)^2}{2\sigma_\sigma^2} \right)\right)
}

Just by looking at the sum of fractions inside the exponential, we see that we will not be able to split the terms up into a quadratic expression in $x$ and a quadratic expression in $\sigma$. This indicates that the resulting joint distribution is not gaussian like we want it to be. Therefore this distribution is not conjugate.

Instead, let us split $p(x, \sigma)$ up into $p(x|\lambda)p(\lambda)$ with the following distributions.

\eq{
  p(\lambda) &\sim \gam{\alpha}{\beta} \\
  p(x|\lambda) &\sim \nml{\mu}{(k\lambda)^{-1}}
}

Now we can carry out the same computation. Hopefully we will find that the result is something in the form of $p(x|\lambda')p(\lambda')$

\eq{
  p(x,\lambda|y) &\propto p(y|x,\lambda) p(x,\lambda) \\
  &= \uvnmlp{y}{x}{\lambda} \uvnmlp{x}{\mu}{(k\lambda)} \gammadist{\lambda}{\alpha}{\beta} \\
  &= \nmz \lambda^{\alpha} e^{-\beta \lambda} \exp{(\nhaf \ \cdot)}
}

We gain one $\lambda$ term from the the normalization terms, but need to spend half of it, so we end up only gaining extra $\haf$ for our gamma distribution.

\eq{
  \alpha' &= \alpha+\haf
}

Inside the exponential, ignoring the $\nhaf$ we have:

\eq{
  & \lambda (y-x)^2 +k \lambda (x-\mu)^2 \\
  &= \lambda y^2 + \lambda x^2 -2y\lambda x + k\lambda x^2 + k\lambda \mu^2 - 2k\lambda \mu x \\
  &= \paren{(1+k)\lambda}x^2 - 2 \paren{(k \mu +  y)\lambda} x + \lambda \paren{y^2+k\mu^2} \\
}

It may seem like we're done here, but we actually have to complete the square.

\eq{
  &= (1+k)\lambda \paren{x^2 - 2 \frac{k\mu+y}{1+k} x} + \lambda \paren{y^2+k\mu^2} \\
  &= (1+k)\lambda \paren{x^2 - 2 \frac{k\mu+y}{1+k} x + \paren{\frac{k\mu+y}{1+k}}^2} + (1+k)\lambda \paren{y^2+k\mu^2} - \lambda \paren{\frac{k\mu+y}{1+k}}^2\\
  &= (1+k)\lambda \paren{x - \frac{k\mu+y}{1+k}}^2 + \lambda \paren{y^2+k\mu^2 - \paren{\frac{(k\mu+y)^2}{1+k}}} \\
  &= (1+k)\lambda \paren{x - \frac{k\mu+y}{1+k}}^2 + \lambda \paren{y^2+k\mu^2 - \frac{(k\mu+y)^2}{1+k}} \\
  &= (1+k)\lambda \paren{x - \frac{k\mu+y}{1+k}}^2 + \lambda \paren{\frac{y^2+ky^2+k\mu^2+k^2\mu^2-(k\mu+y)^2}{1+k}} \\
  &= (1+k)\lambda \paren{x - \frac{k\mu+y}{1+k}}^2 + \lambda \paren{
    \frac{y^2+ky^2+k\mu^2+k^2\mu^2-k^2\mu^2-y^2+2k\mu y}{1+k}
  } \\
  &= (1+k)\lambda \paren{x - \frac{k\mu+y}{1+k}}^2 + \lambda \paren{
    \frac{ky^2+k\mu^2+2k\mu y}{1+k}
  } \\
  &= (1+k)\lambda \paren{x - \frac{k\mu+y}{1+k}}^2 + \lambda \paren{
    \frac{k}{1+k} \paren{y^2+\mu^2+2\mu y}
  } \\
  &= (1+k)\lambda \paren{x - \frac{k\mu+y}{1+k}}^2 + \lambda \paren{
    \frac{k}{1+k} \paren{y+\mu}^2
  } \\
}

By inspection, we can get the update rules for $\beta, \mu, k$

\eq{
  \beta' &= \beta + \haf \frac{k}{1+k} \paren{y+\mu}^2 \\
  \mu' &= \frac{k \mu + y}{k+1} \\
  k' &= k+1
}

So far, we've actually been cheating a bit by only using single sample updates when really, we usually update with an entire dataset.

\eq{
  P(\y|x,\lambda) &= \prod_{y \in \y} \uvnmlp{y}{x}{\lambda} \\
  &= \paren{\frac{\lambda}{2 \pi}}^{\frac{n}{2}} \nexp{\lambda \sum_i (y_i-x)^2} \\
}

\eq{
  p(x,\lambda|\y) &\propto p(\y|x,\lambda) p(x,\lambda) \\
  &= \paren{\frac{\lambda}{2 \pi}}^{\frac{n}{2}} \nexp{\lambda \sum_i (y_i-x)^2} \\
  & \qquad \cdot \uvnmlp{x}{\mu}{(k\lambda)} \gammadist{\lambda}{\alpha}{\beta} \\
  &= \nmz \lambda^{\alpha+\frac{n+1}{2}} e^{-\beta \lambda} \exp{(\nhaf \ \cdot)}
}

Inside the exponential, ignoring the $\nhaf$ we have:

\eq{
  & \lambda \sum_i (y_i-x)^2 +k \lambda (x-\mu)^2 \\
  &= \lambda \sum_i y_i^2 + \lambda n x^2 -2\paren{\sum_i y_i}\lambda x + k\lambda x^2 + k\lambda \mu^2 - 2k\lambda \mu x \\
  &= \paren{(n+k)\lambda}x^2 - 2 \lambda \paren{k \mu + \paren{\sum_i y_i}} x + \lambda \paren{\paren{\sum_i y_i^2}+k\mu^2} \\
}

For convenience, lets define some shorthand terms:

\eq{
  S_y &= \sum_i y_i \\
  S_{y^2} &= \sum_i y^2_i
}

As before, we want to complete the square

\eq{
  &(n+k)\lambda x^2 - 2 \lambda \paren{k \mu + S_y} x + \lambda \paren{S_{y^2}+k\mu^2} \\
  &= (n+k)\lambda \paren{x^2 - 2 \frac{k \mu + S_y}{(n+k)} x} + \lambda \paren{S_{y^2}+k\mu^2} \\
  &= (n+k)\lambda \paren{x^2 - 2 \frac{k \mu + S_y}{(n+k)} x + \paren{\frac{k \mu + S_y}{(n+k)}}^2} \\
  & \qquad + \lambda \paren{S_{y^2}+k\mu^2} - (n+k)\lambda \paren{\frac{k \mu + S_y}{n+k}}^2\\
  &= (n+k)\lambda \paren{x - \frac{k \mu + S_y}{(n+k)}}^2
  + \lambda \paren{S_{y^2}+k\mu^2} - \lambda \frac{(k \mu + S_y)^2}{n+k}\\
  &= (n+k)\lambda \paren{x - \frac{k \mu + S_y}{(n+k)}}^2
  + \lambda \paren{S_{y^2}+k\mu^2 - \frac{(k \mu + S_y)^2}{n+k}}\\
  &= 0
}

Again, we derive the update rules from inspection:

\eq{
  \beta' &= \beta + \haf \paren{S_{y^2}+k\mu^2 - \frac{(k \mu + S_y)^2}{n+k}} \\
  \mu' &= \frac{k\mu + S_y}{n+k} \\
  k' &= k+n
}

I found this alternate update rule for $\beta$ from some lecture notes online, so we will check that they are actually equivalent.

\eq{
  \beta &= \sum_i (y_i - \bar y)^2 + \frac{nk}{n+k}(\bar y - \mu)^2 \\
  &= S_{y^2} - 2 \bar y S_y + n \bar y^2 + \frac{nk}{n+k}\mu^2 + \frac{nk}{n+k} \bar y^2 - 2 \frac{nk}{n+k}\mu \bar y \\
  &= S_{y^2} + k\mu^2 - 2 \bar y S_y + n \bar y^2 - \frac{k^2}{n+k}\mu^2 + \frac{nk}{n+k} \bar y^2 - 2 \frac{nk}{n+k}\mu \bar y \\
  &= S_{y^2} + k\mu^2 - 2 \inv n S_y^2 + \inv n S_y^2 - \frac{k^2}{n+k}\mu^2 + \frac{k}{n(n+k)}  S_y^2 - 2 \frac{k}{n+k}\mu S_y \\
  &= S_{y^2} + k\mu^2 + \paren{\inv n-2 \inv n+\frac{k}{n(n+k)}}S_y^2 - 2\frac{k}{n+k}\mu S_y -\frac{k^2}{n+k}\mu^2 \\
  &= S_{y^2} + k\mu^2 - \frac{1}{n+k}S_y^2 - 2\frac{k\mu}{n+k} S_y -\frac{k^2}{n+k}\mu^2 \\
  &= S_{y^2} + k\mu^2 - \frac{\paren{k\mu + S_y}^2}{n+k}
}

The last thing to deal with is the multivariate gaussian distribution. The likelihood function is:

\eq{
  P(\y|x) &= \prod_i \mvnmlp{y_i}{x}{\Lambda} \\
  &= (2 \pi)^{-\frac{ND}{2}} |\Lambda|^{\frac{N}{2}} \bexp{\nhaf \sum_i \qfo{y_i}{\Lambda}{x}}
}

We see that both the posterior and prior must have a quadratic term in $x$, and a polynomial term in the determinant of $\Lambda$. Mirroring the gaussian-gamma distribution, we try the following prior:

\eq{
  p(\Lambda) &\sim \mathcal{W}(V,m) \\
  p(x) &\sim \nml{\mu}{k \Lambda}
}

The Wishart probability distribution $\mathcal{W}$ characterized as the probability density of $X^TX$ where $X$ is a matrix constructed by drawing $m$ samples from a normal distribution with zero mean and covariance $V$. However, it is possible to derive the wishart distribution simply by looking at the likelihood.

Notice that we need a term polynomial in the determinant of $\Lambda$. Second, we want some function of $\Lambda$ in the exponent. This should match up with the term that results when we complete the square, so it must be a scalar value.

Our strategy will be to multiply the likelihood by the gaussian factor in the prior, and then add on whatever terms are necessary to complete the job, deriving the wishart density along the way.

\eq{
  p(x|\y) &\propto p(\y|x)p(x) \\
  &= p(\y|x)p(x|\mu, k\Lambda)p(L) \\
  &= (2 \pi)^{-\frac{ND}{2}} |\Lambda|^{\frac{N}{2}} \bexp{\nhaf \sum_i \qfo{y_i}{\Lambda}{x}} \\
  &\qquad \cdot \mvnmlp{x}{\mu}{k \Lambda} p(L)\\
  &= \nmz |\Lambda|^{\frac{N+1}{2}} \bexp{\nhaf \sum_i \qfo{y_i}{\Lambda}{x}} \\
  & \qquad \cdot \bexp{\nhaf \qfo{x}{k\Lambda}{\mu} }p(L) \\
}

Lets explore the terms inside the exponential:

\eq{
  &\sum_i \qfo{y_i}{\Lambda}{x} + \qfo{x}{k \Lambda}{\mu} \\
  &= (n+k)\qf{x}{\Lambda} + \sum_i \qf{y_i}{\Lambda} + k\qf{\mu}{\Lambda}
  - 2k\mu^T \Lambda x - 2 x^T \Lambda \sum_i y_i \\
}

Now we can complete the terms with respect to $x$. Given that $A = (n+k)\Lambda$ and $b = k\Lambda \mu + \Lambda S_y$, this comes out to

\eq{
  &\qfo{x}{A}{\minv A b} - \qf{b}{A} + \sum_i \qf{y_i}{\Lambda} - 2k\qf{\mu}{\Lambda} \\
  &= \qfo{x}{(n+k)\Lambda}{\minv{((n+k)\Lambda)}b} - \qf{b}{((n+k)\Lambda)} \\
  &\qquad + \sum_i \qf{y_i}{\Lambda} - 2k\qf{\mu}{\Lambda}\\
}

So the extra terms are
\eq{
  & \sum_i \qf{y_i}{\Lambda} - 2k\qf{\mu}{\Lambda} - \qf{b}{((n+k)\Lambda)}\\
  &= \sum_i \qf{y_i}{\Lambda} - 2k\qf{\mu}{\Lambda} - (n+k) \qf{b}{\Lambda} \\
}

Ideally, we want to combine these quadratic forms into one term, however, this is nontrivial, because we cannot simply add two forms:

\eq{
  \qf{x}{A} + \qf{y}{A} \neq \qf{(x+y)}{A}
}

However, we can play a neat trick by using traces:

\eq{
  \qf{x}{A} &= \tr ( \qf{x}{A}) \\
  &= \tr (xx^T A)
}

Now we can add the forms.

\eq{
  \qf{x}{A} + \qf{y}{A} &= tr\paren{\paren{xx^T + yy^T} A}
}

Similarly, if we define $\Omega$

\eq{
  \Omega &= \sum_i \scov{y_i} + 2k \scov{\mu} - (n+k) \scov{ b } \\
}

Then the extra terms come out to $\tr(\Omega \Lambda)$. Now we know that our prior $p(\Lambda)$ must have something to the effect of $\bexp{\nhaf tr(M\Lambda)}$ in it. Furthermore, $M$ must be a function of $V$, which is the only matrix parameter in the wishart distribution. We choose $M = \minv V$. Therefore the full density function for the wishart is

\eq{
  p(\Lambda) &= \nmz |\Lambda|^{\frac{m}{2}} \bexp{\nhaf \tr (\minv V \Lambda)}
}

Now we may derive the update rules

\eq{
  \mu' &= \frac{1}{n+k} \minv \Lambda b \\
  &= \frac{1}{n+k} \minv \Lambda \paren{k\Lambda \mu + \Lambda S_y} \\
  &= \frac{k\mu + n \bar y}{n+k} \\
  k' &= k+n \\
  V' &= V + \Omega \\
  m' &= m+n
}

I'm reasonably sure these are at least nearly correct, although I haven't bothered to check for myself.

\subsection{Manipulations of Multivariate Gaussians}

Despite these manipulations being pretty nasty to derive, they seem to be very useful, so it is good to know how they can be solved. 

\subsubsection{Partitioned Gaussians}

Given a multivariate gaussian distribution, we want to figure out the marginal distribution when we marginalize out a subset of the variables. We start with a gaussian random variable $x = \vt{x_a}{x_b}$, with mean $\mu = \vt{\mu_a}{\mu_b}$ and precision $\Lambda = \mt{\Lambda_{aa}}{\Lambda_{ab}}{\Lambda_{ba}}{\Lambda_{bb}}$. The probability distribution function is

\eq{
f(x) = (2\pi)^{-\frac{D}{2}} |\Lambda|^{\frac{1}{2}} \exp \left( -\frac{1}{2} (x-\mu)^T \Lambda (x-\mu) \right)
}

We can also expand it out as

\eq{
f \left(\vtx \right) = (2\pi)^{-\frac{D}{2}} \left| \mtlambda \right|^{\frac{1}{2}} \exp \left( -\frac{1}{2} \left( \vtx-\vtmu \right)^T \mtlambda \left(\vtx-\vtmu \right) \right)
}

What we are trying to find is $f(x_a)$. According to the law of total probability

\eq{
f(x_a) = \int f \left(\vtx \right) dx_b
}

To do this, first expand out all the terms in the exponential in $f$. We will also drop the $-\frac{1}{2}$ for now.

\eq{
  \left( \vtx-\vtmu \right)^T \mtlambda \left(\vtx-\vtmu \right)
}

If you expand this out, it becomes
\eq{
&(x_a-\mu_a)^T \Lambda_{aa} (x_a-\mu_a) + (x_a-\mu_a)^T \Lambda_{ab} (x_b-\mu_b)\\
&\qquad +(x_b-\mu_b)^T \Lambda_{ba} (x_a-\mu_a) + (x_b-\mu_b)^T \Lambda_{bb} (x_b-\mu_b)
}


Since we're integrating with respect to $x_b$, it helps to split up the terms into those which contain $x_b$ and those which don't.

The terms that do:
\eq{
x_b^T \Lambda_{bb} x_b - 2 x_b^T \Lambda_{bb} \mu_b + 2 x_b^T \Lambda_{ba} (x_a-\mu_a)
}

The terms that don't:
\eq{
(x_a-\mu_a)^T \Lambda_{aa} (x_a-\mu_a) - 2 (x_a-\mu_a)^T \Lambda_{ab} \mu_b
}

Define $m = \Lambda_{bb}\mu_b - \Lambda_{ba} (x_a-\mu_a)$. Then the first set of terms is
\eq{
  x_b^T \Lambda_{bb} x_b - 2 x_b^T m
  \intertext{We complete the square to obtain}
  \paren{x_b - \Lambda_{bb}^{-1} m}^T \Lambda_{bb} \paren{x_b - \Lambda_{bb}^{-1} m} - m^T \Lambda_{bb}^{-1} m
}

We've pulled out another term out of the terms we said depended on $x_b$, which doesn't contain $x_b$. Namely, $m^T \Lambda_{bb}^{-1} m$. Add this on to the second set of terms:

\eq{
(x_a-\mu_a)^T \Lambda_{aa} (x_a-\mu_a) - 2 (x_a-\mu_a)^T \Lambda_{ab} \mu_b - m^T \Lambda_{bb}^{-1} m
}

Let's call the completed square $B$ and the second set of terms $A$. What we really want to solve is

\eq{
  f(x_a) &= \int f \left(\vtx \right) dx_b \\
  &= \nmz \int \exp \left( -\frac{1}{2} (A+B) \right) dx_b\\
  &= \nmz \exp \left(-\frac{1}{2} A \right) \int \exp \left( -\frac{1}{2} B \right) dx_b
}

Since $B$ is in a quadratic form in $x_b$, we know that the integral is over an unnormalized normal distribution, which evaluates to some constant so we are left with
\eq{
\nmz \exp \left(-\frac{1}{2} A \right)
}

We might suspect that $x_a$ is distributed normally. If this is the case, then we should be able to manipulate $A$ into the form $(x_a - \mu_*)^T \Lambda_* (x_a - \mu_*)$. To make things easier, we're not even going to bother completing the square. Notice that if we expand $(x_a - \mu_*)^T \Lambda_* (x_a - \mu_*)$ we end up with something like

\eq{
x_a^T \Lambda_* x_a - 2 x_a^T \Lambda_* \mu_* + C
}

Where $C$ are some constant terms we don't really care about. Now if we get $A$ into this form, we can simply read off the parameters $A_*$ and $\mu_*$ of our distribution, which is what we want.

I'll reproduce $A$ again.
\eq{
(x_a-\mu_a)^T \Lambda_{aa} (x_a-\mu_a) - 2 (x_a-\mu_a)^T \Lambda_{ab} \mu_b - m^T \Lambda_{bb}^{-1} m
}

Now let's expand $m$ back to full size again.
\eq{
&(x_a-\mu_a)^T \Lambda_{aa} (x_a-\mu_a) - 2 (x_a-\mu_a)^T \Lambda_{ab} \mu_b \\
&\qquad - \left( \Lambda_{bb}\mu_b - \Lambda_{ba} (x_a-\mu_a) \right)^T \Lambda_{bb}^{-1} \left( \Lambda_{bb}\mu_b - \Lambda_{ba} (x_a-\mu_a) \right)
}


To make our job easier, I'm going to play around one of the terms
\eq{
&(x_a-\mu_a)^T \Lambda_{aa} (x_a-\mu_a) - 2 (x_a-\mu_a)^T \Lambda_{ab} \mu_b \\
&\qquad - \left( \Lambda_{bb}\mu_b - (x_a-\mu_a)^T \Lambda_{ba}^T \right)^T \Lambda_{bb}^{-1} \left( \Lambda_{bb}\mu_b - \Lambda_{ba} (x_a-\mu_a) \right)
}
 
Now, we're going to try grabbing out all the quadratic terms. You should find
\eq{
\Lambda_{aa} - \Lambda_{ba}^T \Lambda_{bb}^{-1} \Lambda_{ba}
}

Now $\Lambda_{ba}$ means something vaguely like the relationship between $x_b$ and $x_a$. Because the relationship between $x_b$ and $x_a$ should be the same as the relationship between $x_a$ and $x_b$, it's true that $\Lambda_{ba} = \Lambda_{ab}^T$. But now the quadratic term has coefficient
\eq{
\Lambda_{aa} - \Lambda_{ab} \Lambda_{bb}^{-1} \Lambda_{ba}
}

Therefore the precision matrix is $\Lambda_* = \Lambda_{aa} - \Lambda_{ab} \Lambda_{bb}^{-1} \Lambda_{ba}$. Note that this is equivalent to $\Sigma_{aa}^{-1}$

And now for the linear terms
\eq{
\left( \Lambda_{aa} - \Lambda_{ab} \Lambda_{bb}^{-1} \Lambda_{ba} \right) \mu_a = \Lambda_* \mu_a
}

Reading off that equation from before, this means that $\mu_* = \mu_a$.

To conclude, if we start off with a multivariate gaussian with the variables partitioned into groups $x_a$ and $x_b$, and we ignore all the variables in $x_b$, we end up with another multivariate gaussian with mean $\mu_a$ and covariance $\Sigma_{aa}$.

\subsubsection{Linearly Dependent Gaussians}

The setup is

\eq{
  x &\sim \nml{\mu}{\Lambda^{-1}} \\
  y|x &\sim \nml{Ax+b}{\minv L}
}

We want to find the distribution of $x|y$ and $y$

\eq{
  p(x|y) &= \nmz p(y|x)p(x) \\
  p(y) &= \int p(y|x) dx
}

Let's start with $p(x|y)$.

\eq{
  p(x|y) &\propto \nmz p(y|x)p(x) \\
  &= \mvnmlp{y}{Ax+b}{L} \\
  &\qquad \cdot \mvnmlp{x}{\mu}{\Lambda} \\
}

The terms inside the exponent, dropping the $\nhaf$:

\eq{
  \qfo{y}{L}{(Ax+b)} + \qfo{x}{\Lambda}{\mu} \\
}

Since we are expecting a normal distribution in $x$, we pull out the quadratic terms in $x$:

\eq{
  A^T L A + \Lambda
}

And the linear terms:

\eq{
  -2\paren{A^TL(y-b) + \Lambda \mu}
}

For later convenience, we denote

\eq{
  \minv \Sigma = A^T L A + \Lambda
}

We have now arrived at the distribution $x|y$:

\eq{
 x|y &\sim \nml{\Sigma\paren{A^T L(y-b) + \Lambda \mu}}{\Sigma} \\
}

Now consider $p(y)$. 

\eq{
	p(y,x) &= p(y|x)p(x)
}

We will rewrite the joint distribution of $x$ and $y$ as a partitioned gaussian, and then figure out the partitioned mean and covariance.

Same as before the terms inside the exponent are:

\eq{
  \qfo{y}{L}{(Ax+b)} + \qfo{x}{\Lambda}{\mu} \\
}

We can write out all the second order terms for $x$ and $y$:

\eq{
	y,y &: L \\
    x,x &: A^T L A + \Lambda \\
    x,y &: -A^TL \\
    y,x &: -LA \\
}

So we can rewrite our expression as:

\eq{
	\qf{\vt{y}{x}}{\mt{L}{-LA}{-A^TL}{A^T L A + \Lambda}}
}

Now we see that this looks like the precision matrix of a partitioned gaussian. We want to find the mean and covariance of this partitioned gaussian (or at least the $y$ components of them)

First let us invert the matrix using our matrix inversion identity:

\eq{
	\mt{L}{-LA}{-A^TL}{A^T L A + \Lambda}^{-1} &= \mt{\minv L + \minv L (-LA) M (-A^TL) \minv L}{-\minv L (-LA) M}{-M (-A^TL) \minv L}{M}
    \intertext{where}
    M &= \paren{(A^T L A + \Lambda) - (-A^TL) \minv L (-LA)}^{-1} \\
    &= \paren{(A^T L A + \Lambda) - A^TLA}^{-1} \\
    &= \minv \Lambda \\
    \intertext{Simplifying and substituting in $M$, we get:}
	\mt{L}{-LA}{-A^TL}{A^T L A + \Lambda}^{-1} &= \mt{\minv L + AMA^T}{AM}{MA^T}{M} \\
	&= \mt{\minv L + A \minv \Lambda A^T}{A \minv \Lambda}{\minv \Lambda A^T}{\minv \Lambda} \\
}

Therefore the covariance matrix for $y$ is simply 

\eq{
	\Sigma_{yy} &= \minv L + A \minv \Lambda A^T
}

Now consider the linear terms in $x$ and $y$:
\eq{
	y &: -2Lb \\
    x &: 2A^T L b - 2\Lambda \mu
}

Recall that multiplying the covariance by the linear terms (dropping the $-2$ constant) gives the means:

\eq{
	\vt{\mu_y}{\mu_x} &= \mt{\minv L + A \minv \Lambda A^T}{A \minv \Lambda}{\minv \Lambda A^T}{\minv \Lambda} \vt{Lb}{\Lambda \mu - A^T L b} \\
    &= \vt{\paren{\paren{\minv L + A \minv \Lambda A^T} Lb} + \paren{A \minv \Lambda \paren{\Lambda \mu - A^T L b}}}
    {\paren{\minv \Lambda A^T Lb} + \paren{\minv \Lambda \paren{\Lambda \mu - A^T L b}}} \\
    &= \vt{b + A \minv \Lambda A^T L b + A \mu - A \minv \Lambda A^TLb}
    {\minv \Lambda A^T L (b - b) + \mu} \\
    &= \vt{A \mu + b}{\mu} \\
}

So we get that the mean of $x$ is $\mu$, as should be the case, and the mean of $y$ is $A \mu +b$.

In conclusion, $y$ follows the distribution

\eq{
  y &\sim \nml{A\mu + b}{\minv L + A \minv \Lambda A^T} \\
}

Note that in the simple case where $A = I$ and $b = \vec 0$ we have
\eq{
  x | y & \sim \nml{(L + \Lambda)^{-1} Ly + \Lambda \mu}{(L + \Lambda)^{-1}} \\
  y &\sim \nml{\mu}{\minv L + \minv \Lambda}
}

\subsubsection{Orthogonal marginalization}

Let's consider a formulation for the problem of marginalizing out all but one axis of a multivariate gaussian variable. We will abuse $\nml{}{}$ to mean a probability distribution here. First notice the technique of using the dirac function to write our integral in a nice manner.

\eq{
	f(x) &= \int \nml{\mu}{\Sigma}([x,\x_{-1}]) d\x_{-1} \\
	&= \int \delta(x - \x_1) \nml{\mu}{\Sigma}(\x) d\x \\
}

This formulation is useful if we want to consider the case of projecting a gaussian along a line, or, in other words, marginalizing all directions orthogonal to a certain vector. Formally, we want to compute:

\eq{
	f(a) &= \int \delta(a-w^T \x) \nml{\mu}{\Sigma}(\x) d\x
}

Our line $w$ can be rotated in space so that it lies entirely on one axis. That is, there exists some matrix $R$ such that $Rw$ is non-zero on the first component and 0 on all the other components. Let us rotate both the line and also the gaussian.

\eq{
	f(a) &= \int \delta(a-(Rw)^T \x) \nml{R\mu}{R \Sigma R^T}(\x) d\x
}

Note that $(Rw)^T \x = ||w|| \x_1$ so we have

\eq{
	f(a) &= \int \delta(a-||w|| \x_1) \nml{R\mu}{R \Sigma R^T}(\x) d\x \\
    &=  \int \delta \paren{\frac{a}{||w||} - \x_1} \nml{R\mu}{R \Sigma R^T}(\x) d\x \\
    &=  \int \nml{R\mu}{R \Sigma R^T}([a||w||^{-1}, \x_{-1}]) d\x_{-1} \\
}

So this is simply a scaled version of our toy example of marginalizing all but one variable in a multivariate gaussian.

\subsection{Information Theory}

Information is intuited as the number of bits it takes to represent an event. The entropy of a probability distribution is the expected information of a single event. 

\subsubsection{Entropy}

\eq{
  H[f] &= -E[\log f(x)] & \text{normal entropy}\\
  H[p;q] &= -E_p[\log q(x)] & \text{cross-entropy}\\
  H[x,y] &= -E[\log f(x,y)] & \text{joint-entropy} \\
}

In a discrete probability distribution, entropy does not scale if we move around or scale the discrete events. However, in a continuous distribution, this is not true.

Let $x$ be some variable with known entropy, and suppose we apply a linear transformation to obtain $y = Ax$, so $x = \minv A y$.

The probability distribution of $y$ is
\eq{
  f_y(y) &= f_x(A^{-1}y) \left| \frac{\partial A^{-1}y}{\partial y} \right| \\
  &= f_x(A^{-1}y) |A^{-1}|
}

Now we can proceed to compute the entropy
\eq{
  H[y] &= - \int f_y(y) \log f_y(y) dy\\
  &= - \int f_y(y) \log f_x(A^{-1}y) |A^{-1}| dy \\
  &= - \int f_y(y) \left(\log (f_x(A^{-1}y)) + \log (|A^{-1}|)\right) dy \\
  &= - \int f_y(y) \log (f_x(A^{-1}y)) dy - \int f_y(y) \log (|A^{-1}|) dy\\
  &= - \int f_y(y) \log (f_x(A^{-1}y)) dy - \log |A^{-1}| \\
  &= - \int f_x(A^{-1}y) |A^{-1}| \log (f_x(A^{-1}y)) dy  + \log |A| \\
  &= - \int f_x(x)  |A^{-1}| \log f_x(x) |\frac{\partial y}{\partial x}| dx + \log |A| \\
  &= - \int f_x(x)  |A^{-1}| \log f_x(x) |A| dx + \log |A| \\
  &= - \int f_x(x) \log f_x(x) dx + \log |A| \\
  &= H[x]+ \log |A|
}

There is some intuition to the $\log$ term -- if the amount of space to search is doubled and the density is spread twice as thinly, then we need one more bit to specify the location of some object.

The entropy of the multivariate gaussian can be computed as:

\eq{
  H[x] &= -\int (2\pi)^{-\frac{D}{2}}  | \Sigma |^{-\frac{1}{2}} \exp \left( -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right) \\
  & \qquad \qquad \cdot 
  \log \left( 
    (2\pi)^{-\frac{D}{2}}  | \Sigma |^{-\frac{1}{2}} \exp \left( -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right)
  \right) dx\\
  &=  -\log \left( (2\pi)^{-\frac{D}{2}}  | \Sigma |^{-\frac{1}{2}} \right) \\
  & \qquad -\int (2\pi)^{-\frac{D}{2}}  | \Sigma |^{-\frac{1}{2}} \exp \left( -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right) \\
  & \qquad \qquad \cdot 
  \log \left( 
    \exp \left( -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right)
  \right) dx\\
  &=  \frac{D}{2} \log(2\pi) + \frac{1}{2} \log |\Sigma| \\
  &\qquad - \int (2\pi)^{-\frac{D}{2}}  | \Sigma |^{-\frac{1}{2}} \exp \left( -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right) \\
  &\qquad \qquad \cdot \left( -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right) dx\\
  &= \frac{D}{2} \log(2\pi) + \frac{1}{2} \log |\Sigma| + \frac{1}{2}E\left[ \frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right]\\
  &= \frac{D}{2} \log(2\pi) + \frac{1}{2} \log |\Sigma| + \frac{D}{2}
}

\subsubsection{Mutual Information}

Another useful, if rarely used concept is mutual information

\eq{
  I(x;y) &= H(x) - H(x|y) & \text{and the other way around by symmetry}
}

It can also be defined using KL-divergence.

\eq{
  I(x,y) &= \kl{p(x,y)}{p(x)p(y)} \\
  &= \kl{p(x|y)}{p(x)}
}

These two definitions are equivalent:

\eq{
  I(x,y) &= E\left[\frac{p(x,y)}{p(x)p(y)} \right] \\
  &= E\left[\frac{p(x|y)p(y)}{p(x)p(y)} \right] \\
  &= E\left[\frac{p(x|y)}{p(x)} \right] \\
  &= H(x)-H(x|y) 
}

\subsubsection{KL-Divergence}

KL-divergence is defined below. It is not symmetric but it is always positive. 

\eq{
  \kl{p}{q} &= -E_p \sparen{\log \frac{p(x)}{q(x)}}
}

Note that with respect to $q$, KL-divergence and cross-entropy are equivalent up to a constant factor. In fact:

\eq{
  \kl{p}{q} &= H[p] + H[p;q]
}

The KL-divergence between two gaussian distributions can be analytically derived.
Start with distributions $p(x) = \mathcal{N}(x|\mu, \sigma^2)$ and $q(x) = \mathcal{N}(x|m, s^2)$.

\eq{
  D_{KL}(p||q) &= E_p\left[\log \frac{p(x)}{q(x)} \right] \\
  &= E_p\left[\log \frac{
      \frac{1}{\sqrt{2 \pi \sigma^2}} 
      \exp \left( -\frac{(x-\mu)^2}{2\sigma^2} \right) }
    {\frac{1}{\sqrt{2 \pi s^2}} 
      \exp \left( -\frac{(x-m)^2}{2s^2} \right)}  
  \right]\\
  &= E_p\left[\log \left( \frac{
        \sqrt{2 \pi s^2} }{
        \sqrt{2 \pi \sigma^2}
      }
      \exp \left(\frac{(x-m)^2}{2s^2}-\frac{(x-\mu)^2}{2\sigma^2} \right) \right) \right]\\
  &= E_p\left[\log\left(\frac{s}{\sigma}\right) + 
    \left(\frac{(x-m)^2}{2s^2}-\frac{(x-\mu)^2}{2\sigma^2} \right) \right]\\
  &= \log\left(\frac{s}{\sigma}\right)
  + E_p\left[\frac{(x-m)^2}{2s^2}\right]-E_p\left[\frac{(x-\mu)^2}{2\sigma^2} \right]\\
  &= \log\left(\frac{s}{\sigma}\right)
  + \frac{E_p [(x-m)^2]}{2s^2}
  - \frac{E_p[(x-\mu)^2]}{2\sigma^2}\\
  &= \log\left(\frac{s}{\sigma}\right)
  + \frac{E_p [(x-m)^2]}{2s^2}
  - \frac{\text{Var}_p[x]}{2\sigma^2}\\
  &= \log\left(\frac{s}{\sigma}\right) - \frac{1}{2}
  + \frac{E_p [(x-m)^2]}{2s^2} \\
  &= \log\left(\frac{s}{\sigma}\right) - \frac{1}{2}
  + \frac{E_p [x^2]-E[2mx]+E[m^2]}{2s^2} \\
  &= \log\left(\frac{s}{\sigma}\right) - \frac{1}{2}
  + \frac{\mu^2 + \sigma^2 - 2m\mu + m^2}{2s^2} \\
}

Let's try the multivariate gaussian as well

\eq{
  p(x) &= (2\pi)^{-\frac{D}{2}} |\Sigma|^{-\frac{1}{2}} 
  \exp \left(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right) \\
  q(x) &= (2\pi)^{-\frac{D}{2}} |L|^{-\frac{1}{2}} 
  \exp \left(-\frac{1}{2} (x-m)^T L^{-1} (x-m) \right) \\
  \intertext{Then the KL divergence is}
  &E_p\left[\log \frac{p(x)}{q(x)} \right] \\
  &= E_p\bigg[ \log \frac{
      (2\pi)^{-\frac{D}{2}} |\Sigma|^{-\frac{1}{2}} 
      \exp \left(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right)
    }{
      (2\pi)^{-\frac{D}{2}} |L|^{-\frac{1}{2}} 
      \exp \left(-\frac{1}{2} (x-m)^T L^{-1} (x-m) \right)
    }  
  \bigg]\\
  &= E_p \bigg[ \frac{1}{2} \log |L| - \frac{1}{2} \log |\Sigma| \\
  	& \qquad \qquad +
    \log \left(
      \exp \left(
        \frac{1}{2} \left( (x-m)^T L^{-1} (x-m) - (x-\mu)^T \Sigma^{-1} (x-\mu) \right)
      \right) \right) \bigg]\\
  &= E_p \bigg[ \frac{1}{2} \log |L| - \frac{1}{2} \log |\Sigma| \\
  	& \qquad \qquad +
    \frac{1}{2} \left( (x-m)^T L^{-1} (x-m) - (x-\mu)^T \Sigma^{-1} (x-\mu) \right)
  \bigg]\\
  &= \frac{1}{2} E_p \left[ \log |L| - \log |\Sigma| + 
    (x-m)^T L^{-1} (x-m) - (x-\mu)^T \Sigma^{-1} (x-\mu)
  \right]
}

We derive the expectation of a quadratic form in another section, but it gives us the following.
\eq{
  E[(x-m)^T L^{-1} (x-m)] &= tr(L^{-1} \Sigma) + (\mu-m)^T L^{-1} (\mu-m) \\
  E[(x-\mu)^T \Sigma^{-1} (x-\mu)] &= tr(\Sigma^{-1} \Sigma) + \mu^T \Sigma^{-1} \mu \\
  &= D
}

Now we can plug these back into where we left off.
\eq{
D_{KL}(p||q) &= \frac{1}{2} \left( \log |L| - \log |\Sigma| + 
tr(L^{-1} \Sigma) + (\mu-m)^T L^{-1} (\mu-m) - D \right)
}

\subsubsection{Fisher Information}

The fisher information matrix $F(\theta)$ is defined as

\eq{
  F_{ij}(\theta) &= E_q \sparen{\parf{\log q(x)}{\theta_i} \parf{\log q(x)}{\theta_j}}
}

for some parameterized probability distribution $q$. 

There are many confusing interpretations of the fisher information matrix.

In my opinion, it is best seen as the hessian of the KL-divergence.

\eq{
  \kl{p}{q_\theta} &= -E_p[\log \frac{p(x)}{q(x)}] \\
  &= -E_p[\log p(x) - \log q(x)] \\
  \hess_\theta \kl{p}{q_\theta} &= -\hess_\theta E_p[\log p(x) -\log q_\theta(x)] \\
  &= \hess_\theta E_p[\log q_\theta(x)] \\
  &= E_p[\hess_\theta \log q_\theta(x)] \\
  &= E_q \sparen{ \parf{\log q(x)}{\theta_i} \parf{\log q(x)}{\theta_j}}
}

\subsubsection{Coding Theory}

Say you manage to train a model to predict the next character some text, and your cross-entropy loss comes out to 1 bit per character. How can you utilize this to compress an 1000 character text file to a 1000 bits? Notice that there are only $256^{1000}$ possible ascii strings of length 1000. In addition, your model defines $p(x)$ for each such string $x$. Finally, we know that the sum of all $p(x)$ is 1. Therefore, it is possible to split the unit length line segment into $256^{1000}$ intervals, each with length $p(x)$.

Notice that to find a single message/interval, it suffices to name a single point in that interval. Intuitively, intervals half as likely are twice as small and require on average, one more binary digit in order to indicate their position on the line segment. Now it remains to prove that an interval of size $p$ at most $- \lceil \lg p \rceil$ binary digits to specify. This is clearly true because $p > 0.5$ implies only one bit is needed, $p > 0.25$ implies only two bits are needed, etc, by pigenohole principle of sorts. This scheme is known as arithmetic coding.

Using this naive scheme is not very efficient, because enumerating out $256^{1000}$ intervals could take a long time. However, it is actually possible to find the position of a given interval in much less time, which speeds encoding. The key is to split the unit interval up into $m$ intervals, for $m$ different symbols. Then, split each of these intervals up into $m$ subintervals, and so on. The $i$th layer of intervals corresponds to the $i$th symbol in the message we want to encode.

\section{Linear Algebra}
\subsection{Identities and Manipulations}

\subsubsection{Completing the Square}

The scalar case:
\eq{
  x^2-2x &= x^2-2x+1-1 = (x-1)^2 -1
}

A more general form:
\eq{
  ax^2-2bx &= a\paren{x^2-2\frac{b}{a}x} \\
  &= a\paren{x^2-2\frac{b}{a}x + \paren{\frac{b}{a}}^2} - a\paren{\frac{b}{a}}^2\\
  &= a\paren{x - \frac{b}{a}}^2 - \frac{b^2}{a}\\
}

Now let's try the vector/matrix formula, replacing division with multiplcation by the inverse matrix. We will work backwards.

\eq{
  & \qfo{x}{A}{\minv A b} - \qf{b}{A} \\
  &= \qf{x}{A} + \qf{(\minv A b)}{A} - 2x^T A (\minv A b) - \qf{b}{A} \\
  &= \qf{x}{A} + b^T A^{-1^T} A\minv A b - 2x^Tb - \qf{b}{A} \\
  &= \qf{x}{A} + b^T A^{-1^T} b - 2x^T b - \qf{b}{\minv A} \\
  &= \qf{x}{A} - 2b^T x \\
}

This completes the proof. Notice that we played a bit loosely with the matrix $A$, assuming that is was symmetric, which means the inverse is also symmetric. Symmetric matrices are usually a safe assumption for quadratic coefficients, because every non-symmetric quadratic form can be converted into a symmetric one.

To be precise, we say that an anti-symmetric matrix is one such that $A = -A^T$. Then if we can decompose every matrix into the sum of a symmetric and anti-symmetric one, we have

\eq{
  \qf{x}{A} &= \qf{x}{A_s} + 2\qf{x}{A_a} \\
  &= \qf{x}{A_s} + \qf{x}{A_a} + \qf{x}{A_a^T} \\
  &= \qf{x}{A_s} + \qf{x}{A_a + A_a^T} \\
  &= \qf{x}{A_s}
}

Where $A_s$ is the symmetric component and $A_a$ is half the anti-symmetric component.

This decomposition is relatively simple:

\eq{
  A_s &= \frac{A + A^T}{2} \\
  2A_a &= A-A_s \\
  &= A-\frac{A+A^T}{2} \\
  &= \frac{A-A^T}{2}
}
It shouldn't be too hard to verify that $A_s$ and $A_a$ are symmetric and antisymmetric respectively.

\subsubsection{Partitioned Inverse}

\eq{
  \mt{A}{B}{C}{D}^{-1} &= \mt{\minv A+\minv A B M C \minv A}{-\minv A B M}{-MC \minv A}{M}
  \intertext{where $M$ is defined by}
  M &= \minv{\paren{D-C \minv A B}}
}

Let's use this to show the result from the section on partial margianalization of gaussians.

\eq{
  \Sigma_{aa} = \minv{\paren{\Lambda_{aa} - \Lambda_{ab} \Lambda_{bb}^{-1} \Lambda_{ba}}}
}

At first glance, it doesn't look like the RHS of this expression shows up anywhere in our identity. Particularly, since we are looking at $\Sigma_{aa}$, we would maybe expect the RHS to show up as the entry in the top left block of the RHS of the identity. Therefore, the LHS of the identity must be $\Lambda$, and the RHS must be $\Sigma^{-1}$.

Therefore we have
\itmz{
\item $A = \Sigma_{aa}$
\item $B = \Sigma_{ab}$
\item $C = \Sigma_{ba}$
\item $D = \Sigma_{bb}$
}

According to the identity, then 

\eq{
  &\minv{\paren{\Lambda_{aa} - \Lambda_{ab} \Lambda_{bb}^{-1} \Lambda_{ba}}} \\
  &= \minv{\paren{
      \paren{\minv{\Sigma_{aa}}+\minv{\Sigma_{aa}} \Sigma_{ab} M \Sigma_{ba} \minv{\Sigma_{aa}}} -
      \paren{-\minv{\Sigma_{aa}} \Sigma_{ab} M}
      \paren{M}^{-1}
      \paren{-M \Sigma_{ba} \minv{\Sigma_{aa}}}}} \\
  &= \minv{\paren{
      \paren{\minv{\Sigma_{aa}}+\minv{\Sigma_{aa}} \Sigma_{ab} M \Sigma_{ba} \minv{\Sigma_{aa}}} -
      \paren{\minv{\Sigma_{aa}} \Sigma_{ab} M \Sigma_{ba} \minv{\Sigma_{aa}}}
    }} \\
  &= {\Sigma_{aa}^{-1}}^{-1} \\
  &= \Sigma_{aa}
}

This completes the proof.

\subsubsection{Trace of an inverse matrix}

Consider a real symmetric positive definite matrix $A$. How can $\tr(\minv A)$ be computed? First notice that it is always possible to find the eigendecomposition (from a later section) $A = P D \minv P$ for some $P$ and diagonal $D$. 

\eq{
	\minv A &= (P D \minv P)^{-1} \\
    &= P (PD)^{-1} \\
    &= P \minv D \minv P
    \tr(\minv A) &= \tr(P \minv D \minv P) \\
    &= \tr(\minv P P \minv D) \\
    &= \tr(\minv D)
}

By definition of eigendecomposition, the values on the diagonal of $D$ are the eigenvalues of $A$, and therefore the values on the diagonal of $\minv D$ are the inverse of the eigenvalues. Therefore, if $\lambda_i$ are the eigenvalues of $A$, then:

\eq{
	\tr(\minv A) &= \sum_i \frac{1}{\lambda_i}
}

\subsection{Decompositions and Factorizations}

A matrix decomposition or a matrix factorization is the process of finding a product of matrices which equal the matrix we are decomposing or factoring.

\subsubsection{Singular Value Decomposition}

\eq{
  M &= U \Sigma V^T
}

Singular value decomposition is best understood as performing any linear transformation in three steps. In particular, pretend we are applying a transformation on some parallelogram.

\enum{
\item rotate the current shape
\item rescale shape along its axes
\item rotate the shape again to get a new parallelogram
}

Any linear transformation can be decomposed into these three steps. To be more formal, if $M$ is an $m$ by $n$ matrix, we can say the following:

\enum{
\item apply $V^T$, an $n$ by $n$ matrix, which is orthogonal (since we are rotating)
\item apply $\Sigma$, an $m$ by $n$ matrix with only entries on the diagonal (since it does scaling)
\item apply $U$, an $m$ by $m$ matrix, which is orthogonal
}

One may wonder why we need three steps. Is a single rotation followed by a scaling insufficient? Consider the task of constructing a kite-shape which is not axis-aligned.

It is relatively easy to invert matrices once the SVD has been found:

\eq{
  \minv M &= V(U\Sigma)^{-1} \\
  &= V\minv{\Sigma}U^T \\
}

If $n = m$, then $\Sigma$ can be inverted by inverting each number on the diagonal. Otherwise, $\Sigma$ is inverted by inverting the diagonal and then transposing.

\subsubsection{Eigendecomposition}

\eq{
  M &= Q\Lambda \minv Q
}

Eigendecomposition factorizes a matrix into three other matrices, where $Q$ contains column-wise eigenvectors, and $\Lambda$ contains eigenvalues. The process of decomposition is not too difficult to see:

\eq{
  Mq_i &= \lambda_i q_i \\
  MQ &= [ \lambda_1 q_i, \lambda_2 q_2, \cdots ] \\
  &= Q\Lambda \\
  M &= Q\Lambda \minv Q
}

\subsubsection{Spectral Theorem}

Spectral theorem says that the matrix $Q$ from above can be orthonormal if $M$ is a real symmetric matrix. Therefore we can write

\eq{
  M &= Q\Lambda Q^T
}

\subsection{Principle Component Analysis}

The goal of PCA should find a new orthogonal basis for a set of data. It should be an ordered basis, such that the first basis has the highest possible variance, the second basis the second highest possible variance, and so on. Draw a picture if you don't understand why this is a great idea. 

To make this more mathematically concrete, the covariance matrix is $\cov \propto XX^T$ if $X$ has datapoints as columns. We want the unit vectors $p_1, ... p_n$ which maximize $\qf{p_i}{C}$. We will solve this with lagrange multipliers.

\eq{
  \min \qf{p_i}{C} \\
  \st \forall j < i: p_i^T p_j = 0 \\
  p_i^T p_i = 0 \\
}

We can write the lagrangian:

\eq{
  \lag (p_i, \lambda) &= \qf{p_i}{C} + \lambda\sos{p_i} + \sum_j \lambda_i p_i^T p_j \\
  \grad \lag &= 2Cp_i + 2\lambda p_i + \sum_j \lambda_j p_j \\
}

\eq{
  0 &= 2C p_i + 2\lambda p_i + \sum_j \lambda_j p_j \\
  &= 2 \paren{\sum_j \lambda_j p_j}^T C p_i + 2 \sum_j \lambda_j p_j^T p_i + \sos{\paren{\sum_j \lambda_j p_j}} \\
  &= 2 \paren{\sum_j \lambda_j C p_j}^T p_i + \sos{\paren{\sum_j \lambda_j p_j}} 
}

To proceed further, we must assume that $p_j$ are eigenvectors. We will then show that by induction, $p_i$ is also an eigenvector, which makes our assumption reasonable. Finally, there is the base case that $p_1$ is an eigenvector (we will not show this).

\eq{
  &= 2 \paren{\sum_j \lambda'_j p_j}^T p_i + \sos{\paren{\sum_j \lambda_j p_j}} \\
  &= \sos{\paren{\sum_j \lambda_j p_j}} \\
  &\leq \sum_j \lambda_j^2 \sos{p_j}
}

It is now clear that $\lambda_j = 0$. Therefore our problem is reduced to

\eq{
  0 &= \grad \lag \\
  &= 2Cp_i + 2\lambda p_i \\
  C p_i &= -\lambda p_i 
}

So it is not apparent that $p_i$ is an eigenvector. Moreover, since we are trying to maximize our expression, $p_i$ is the eigenvector with the largest value that does not violate the orthogonality constraints. Therefore, it must be the largest eigenvector we have not picked yet.

Note that symmetric matrices have orthogonal eigenvectors. 

Next is the question of finding the eigenvectors and values of a matrix. We will begin by performing SVD on the data matrix $X$.

\eq{
  X &= U\Sigma V^T \\
  \cov &= XX^T \\
  &= (U\Sigma V^T)(U\Sigma V^T)^T \\
  &= (U\Sigma V^T)(V \Sigma^T U^T) \\
  &= U \Sigma \Sigma^T U^T \\
}

Note that this looks remarkably like an eigendecomposition of the covariance matrix, which means $\Sigma \Sigma^T$ contains the eigenvalues of $\cov$ and $U$ contains the respective eigenvectors, which is exactly what we want. To be precise, the decompositions are equivalent only up to permutation of eigenvectors and values, but that is sufficient for us.

\subsection{Expectation of Quadratic Forms}

Suppose $x \sim \nml{\mu}{\Sigma}$, how can we compute the expectation $\qf{x}{A}$?

\eq{
  E[\qf{x}{A}] &= \tr \paren{E[\qf{x}{A}]} \\
  &= \tr \paren{E[Axx^T]} \\
  &= \tr \paren{AE[xx^T]} \\
  \intertext{we claim $E[xx^T] = \Sigma + \mu\mu^T$}
  &= \tr \paren{A(\Sigma+\mu \mu^T)} \\
  &= \tr(A\Sigma) + tr(A\mu\mu^T) \\
  &= \tr(A\Sigma) + \qf{\mu}{A} \\
}

Now it remains to prove that $E[xx^T] = \Sigma + \mu\mu^T$

\eq{
  E[xx^T]_{ij} &= E[x_i x_j] \\
  &= E[(x_i-\mu_i)(x_j-\mu_j) - \mu_i \mu_j + x_i\mu_j + x_j \mu_i] \\
  &= \Sigma_{ij} - \mu_i\mu_j + \mu_j E[x_i] + \mu_i E[x_j] \\
  &= \Sigma_{ij} - \mu_i \mu_j + 2 \mu_i \mu_j \\
  &= \Sigma_{ij} + \mu_i \mu_j
}

\section{Probabilistic Graphical Models}
\subsection{Forward Backward Algorithm}

The key to deriving the forward backward algorithm for the linear-chain CRF is to realize that there is no fundamental difference between HMM and CRF.
The likelihood of a linear-chain CRF is defined to be the product of all the potentials. But before we get mixed up in math here: $o_i$ denotes observation and $x_i$ denotes the hidden state.

\eq{
P(\x) = \prod_i \phi(x_i) \prod_{i,i+1} \phi(x_i, x_{i+1})
}

On the other hand, the likelihood of the equivalent HMM model is the following:

\eq{
P(x) = \prod_i P(o_i | x_i) P(x_i | x_{i+1})
}
Note this product is incorrect when $i = n$, but let's ignore that.

Now if you squint really closely, you're realize that these two equations are actually the same thing. We can just replace $P(o_i|x_i)$ with $\phi(x_i)$ and $P(x_i|x_{i+1})$ with $\phi(x_i, x_{i+1})$. So it suffices to solve inference on the general setting of CRFs. However, we'll start on HMMs simply to avoid a bit of headaches.

We want to solve for $P(x_k|\o)$. The key is the following manipulation:

\eq{
  P(x_k|\o) &= \frac{P(\o|x_k)p(x_k)}{P(\o)} \\
  &\propto P(\o|x_k)p(x_k) \\
  &= P(\o_{1:k}|x_k)P(\o_{k+1:n}|x_k)P(x_k) \\
}

Let's take that first term and apply Bayes' Theorem again

\eq{
  P(\o_{1:k}|x_k) &= \frac{P(x_k | \o_{1:k}) P(\o_{1:k})}{P(x_k)} \\
  &\propto \frac{P(x_k | \o_{1:k})}{P(x_k)}
}

And substitute it back in...

\eq{
  P(\o_{1:k}|x_k)P(\o_{k+1:n}|x_k)P(x_k) &= \frac{P(x_k|\o_{1:k})}{P(x_k)}P(\o_{k+1:n}|x_k)P(x_k) \\
  &= P(x_k|\o_{1:k})P(\o_{k+1:n}|x_k)
}

So now we just need to figure out how to compute $P(x_k|\o_{1:k})$ and $P(o_{k+1:n}|x_k)$. We can do this inductively. To do this, we specify that each $x_i$ is a categorical variables in one of $m$ classes indexed by $j$. In addition, we need $\pi$, the initial distribution of $x_1$ and $T$ being $P(x_{k+1|k})$ and also $Q$ being $P(o_i|x_k)$. 

Our base case $P(x_1|o_1)$ can be solved with Bayes' Theorem

\eq{
  P(x_1|o_1) &= \frac{P(o_1|x_1)P(x_1)}{P(x_1)} \\
  &\propto P(o_1|x_1)P(x_1) \\
  &= Q\pi
}

Then we can simply normalize $Q\pi$ to be a probability vector. Now we will show the inductive step.

\eq{
  P(x_k|\o_{1:k}) &= P(x_k|o_k,\o_{1:k-1}) \\
  &= \frac{P(o_k|x_k,\o_{1:k-1})P(x_k|\o_{1:k-1})}{P(o_k|\o_{1:k-1})} \\
  &\propto P(o_k|x_k,\o_{1:k-1})P(x_k|\o_{1:k-1}) \\
  &= P(o_k|x_k)P(x_k|\o_{1:k-1}) \\
  &= P(o_k|x_k) \sum_j P(x_k|x_{k-1} = j)P(x_{k-1}=j|\o_{1:k-1}) \\
}

All of these quantities are clearly obtainable. We must simply normalize over values of $x_k$ in order to get a proper probability distribution.

In the backward case, we start off with the base case $P(o_n|x_{n-1})$

\eq{
  P(o_n|x_{n-1}) &= \sum_j P(o_n|x_n = j)P(x_n=j|x_{n-1})
}

And for the inductive case we have

\eq{
  P(o_{k+1:n}|x_k) &= \sum_j P(o_{k+1:n}|x_{k+1}=j)P(x_{k+1}=j|x_k) \\
  &= \sum_j P(\o_{k+2:n}|x_{k+1}=j)P(o_{k+1}|x_{k+1} = j)P(x_{k+1}=j|x_k) \\
}

Again, all of these quantities are computable, one via induction, so we are finished here. For CRFs, since factors are not necessarily probabilities, we can carry out an extra normalization step at the end.

\subsection{Message Passing}

Some notation:
\itmz{
  \item $x$ is the variable node for which we want the marginal distribution
  \item $\x$ is the total set of variables in the graphic
  \item $f_s$ refers to a factor which is a neighbor of $x$
  \item $x_i'$ refers to the neighbors of $f_s$ excluding $x$.
  \item $f'_k$ refers to a factor which is a neighbor of $x'$
  \item $X_s$ refers to the set of variables $x$ which are below $f_s$
  \item $X_j$ refers to the set of variables $x$ which are below $x_j$ exclusive.
  \item $\msg{s}{x}$ refers to a message from $s$ to $x$.
}

Furthermore, use $F_s(x_j, X_s)$ to denote the product of the factors in the subgraph rooted at $f_s$. Similarly, $G_j(x_j, X_j)$ is used to denote the product of factors in the subgraph rooted at $x_j$. 

We want to solve for the distribution $p(x)$.

\eq{
  p(x) &= \sum_{\x \setminus x} p(\x) \\
  &= \prod_{s \in N(x)} \sum_{X_s} F_s(x, X_s) \\
}

Now note that $\sum_{X_s} F_s(x, X_s)$ is a quantity each factor node $s$ could compute by itself, so it is a good candidate for being a message. Let's say that

\eq{
  \msg{s}{x} &= \sum_{X_s} F_s(x, X_s)
  \intertext{or more generally that}
  \msg{s}{x_i} &= \sum_{X_s} F_s(x_i, X_s)
}

Then we can substitute this back into the equation fork
\eq{
  p(x) &= \prod_{s \in N(x)} \msg{x}{s}
}

It remains to derive an algorithm for computing $\sum_{X_s} F_s(x, X_s)$ efficiently. First let us derive an expansion for $F_s$.

\eq{
  F_s(x, X_s) &= f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} G_j(x_j, X_j) \\
}

Let $X_s'$ denote $X_s \setminus N(s)$. 

\eq{
  \msg{s}{x} &= \sum_{X_s} f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} G_j(x_j, X_j) \\
  &= \sum_{\ne{s}{x}} \sum_{X_s'} f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} G_j(x_j, X_j) \\
  &= \sum_{\ne{s}{x}} f_s(x, \ne{s}{x}) \sum_{X_s'} \prod_{j \in \ne{s}{x}} G_j(x_j, X_j) \\
  &= \sum_{\ne{s}{x}} f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} \sum_{X_j} G_j(x_j, X_j) \\
}

Just like we noticed that the sum of $F_s$ might be a good candidate for being a message, the sum of $G_j$ is a good candidate, because it can be computed by variable $j$ with no knowledge of anything else.

Let $\msg{j}{s} = \sum_{X_j} G_j(x_j, X_j)$. Then our equation from beforehand becomes

\eq{
  \msg{s}{x} &= \sum_{\ne{s}{x}} f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} \msg{j}{s} \\
}

So in order for factor $s$ to send variable $x$ the message, it must collect the messages from it's neighbors excepting $x$, take the product, and then marginalize over all variables except $x$.

But what are the messages from it's neighbors? How do we compute $G_j(x_j, X_j)$? Notice that $G_j$ is simply the product of all factors below it, which is the product of the factors in each subtree.

\eq{
  G_j(x_j, X_j) &= \prod_k F_k(x_j, X_k)
}

So now we can compute our message: 

\eq{
  \msg{j}{s} &= \sum_{X_j} G_j(x_j, X_j) \\
  &= \sum_{X_j} \prod_{k \in \ne{j}{s}} F_k(x_j, X_k) \\
  &= \prod_{k \in \ne{j}{s}} \sum_{X_k} F_k(x_j, X_k) \\
  &= \prod_{k \in \ne{j}{s}} \msg{k}{j}
}

So in order for a variable $x_j$ to send factor $f_s$ a message, it must collect the messages from it's child factors, then take the product of them all. Now the whole algorithm is clear. Starting from the leaves, we propagate messages upwards according to these rules until we reach node $x$. At $x$, we simply take the product of all incoming messages and read off the marginal probabilities.

There is just one issue -- the matter of the base cases. When the leaf is a variable, we can see that the product of the incoming messages (there are none) is always 1. Then according to the formula, the value is simply $f(x_l)$.

When the leaf is a factor, the outgoing message is simply the product of all incoming messages, which is always 1. Then the message is also 1.

Another detail is whether there is a more efficient strategy to computing the marginal probabilities of all the nodes (not just one) instead of having to repeat this process once for every single node in the graph, which adds on a factor $n$ complexity.

All that is required to compute the marginal of a given node is that the surrounding factors have sent their message to that factor. Therefore, we can simply run the algorithm above to find the marginal for the root, and then have every variable and factor send messages downwards, starting from the root. This enables every variable node to compute its marginal.

Another small detail is that we actually send $k$ messages at a time, one for each value of each variable $x_i$. 

How can we generalize this algorithm to general graphs, possibly with cycles? Well it turns out that if we initialize all the messages to 1, and then iteratively run this algorithm to simultaneously update all messages, you eventually get the right answer. Maybe. 

\section{Linear and Nonlinear Bayesian Models}

The Bayesian models we discuss all operate in the same way. First, we have a prior on the weights/parameters of the model. Then, we can describe the likelihood of the data given the parameters. This gives us a posterior distribution on the parameters. Finally, we can solve for the predicative distribution by integrating predictions across the posterior. 

\subsection{Bayesian Linear Regression}

We consider the example where each data-point is $x \in \RR^n$ and each target is $y \in \RR$. Therefore our weight matrix $w$ will be shaped $m$ by $n$. Our model for data generation is

\eq{
	y_i|w &\sim \nml{x_i^T w}{\beta^{-1}} \\
    \vec y | w &\sum \nml{X^T w}{\beta^{-1} I}
}

where matrix $X$ has all the datapoints $x_i$ as columns concatenated together.

Therefore the likelihood comes out to

\eq{
	p(y|w) &= \prod_i \uvnmlp{y_i}{w^T x_i}{\beta^{-1}} \\
    &= \nmz \bexp{ -\frac{\beta}{2} \sum_i (y_i-w^T x_i)^2 } \\   
    &= \nmz \bexp{ -\frac{\beta}{2} \paren{\sum_i y_i^2 + \sum_i (w^T x_i)^2 - 2 \sum_i y_i w^Tx_i } } \\
    &= \nmz \bexp{ -\frac{\beta}{2} \paren{\sum_i w^T x_i x_i^T w - 2  w^T\sum_i y_i x_i } } \\
    &= \nmz \bexp{ -\frac{\beta}{2} \paren{\qf{w}{\paren{\sum_i x_i x_i^T}} - 2  w^T\sum_i y_i x_i } } \\
}

This is normal in $w$. Recall that the conjugate prior of the normal distribution is another normal distribution. Therefore we use the prior:

\eq{
	w &\sim \nml{0}{\alpha^{-1} I } \\
}

Now consider the distribution of $w$ and $\vec y | w$. This is a linear gaussian model which we have solved before. 

\eq{
    \minv \Sigma &= X (\beta I) X^T + \alpha I \\
    &= \beta XX^T + \alpha I \\
    w|y &\sim \nml{\Sigma (X (\beta I) y + \alpha^{-1} I \mu)}{\Sigma} \\
    &\sim \nml{\beta \Sigma X y}{\Sigma}
}

Now that we have a posterior on the weights, we can find a predicative distribution for some new data-point $x'$ and the associated target $y'$

\eq{
	y'|w,y &\sim \nml{x'^T w}{\beta^{-1}}
}

We can find the distribution of $y'$, as we have derived in the section on linear gaussian models.

\eq{
	y' &\sim \nml{x'^T \mu_w}{\beta^{-1} + x'^T \Sigma x'}
	\intertext{where $\mu_w$ is the previously derived}
	\mu_w &= \beta \Sigma X y \\
	y' &\sim \nml{\beta x'^T \Sigma X y}{\beta^{-1} + x'^T \Sigma x'}
}

\subsubsection{Evidence Approximation}

In the previous section, we had fixed values for $\alpha$ and $\beta$. However, to have a fully bayesian model, we should give these variables a prior distribution as well.

\eq{
  w | \alpha &\sim \nml{0}{\alpha^{-1} I} \\
  \alpha, \beta &\sim p(\alpha, \beta) \\
}

We want to solve for the distribution of $y'$

\eq{
  p(y') &= \int \int p(y'|\alpha, \beta) p(\alpha, \beta) \, d\alpha\, d\beta
}

Recall that $p(y'|\alpha, \beta)$ is a known gaussian distribution.

Rather than try to integrate this expression, the evidence approximation makes the following claim: $p(y'|\alpha, \beta)$ is relatively small for most $\alpha$ and $\beta$, and peaks sharply at some optimal values $\alpha^*$, $\beta^*$. Therefore, we can approximate

\eq{
  p(y') &= p(y'|\alpha^*, \beta^*)
}

The values $\alpha^*$ and $\beta^*$ can be found the obvious way -- by maximizing the likelihood of the data.

\eq{
  \alpha^*, \beta^* = \argmax_{\alpha, \beta} p(y|\alpha, \beta)
}

Note that we can't simply take the posterior on $w$ -- let's call it $q$, and claim that

\eq{
  p(y|\alpha, \beta) = \int p(y|w, \beta)q(w|\alpha)\, dw
}

This is incorrect because

\eq{
  p(y|\alpha, \beta) &= \int p(y|w, \beta)p(w|\alpha)\, dw
}

And in the previous attempt, we took $p(w|\alpha)$ to be $p(w|y, \alpha, \beta)$ and allowed ourselves to cheat by looking at the data. We will evaluate the integral. First be very careful that 

\eq{
	p(y|\alpha, \beta) &\neq \prod_i p(y_i | \alpha, \beta)
}

because each $y_i$ influences the probability of $w$, which changes the probability of another $y_j$. Therefore our integrand is

\eq{
	\paren{ \prod_i p(y_i | w, \beta) } p(w|\alpha) 
}

We will solve this integral by completing the square. However, unusually, we will need to carefully keep track of all the constant terms as well.

\eq{
	\mvnmlp{w}{0}{\alpha I} \prod_i \uvnmlp{y_i}{w^T x_i}{\beta} 
}

Let's complete the square of the term inside the exponent:
\eq{
	&\beta \sum_i (y_i-w^T x_i)^2 + \alpha w^T w \\
    &= \beta (y-X^T w)^T (y-X^T w) + \alpha w^T w \\
    &= \beta y^Ty - 2\beta y^T X^T w + \qf{w}{(\beta XX^T + \alpha I)} \\
    &= \beta y^Ty - 2\beta y^T X^T w + \qf{w}{\minv \Sigma} \\
    &= \qf{(w - \beta \Sigma X y)}{\minv \Sigma} + \qf{(\beta Xy)}{ \Sigma} + \beta y^T y
\intertext{Notice that $\beta \Sigma X y$ was the mean of the posterior on $w$ from before. Call it $\hat w$}
 	&= \qf{(w- \hat w)}{\minv \Sigma} - \qf{(\beta Xy)}{ \Sigma} + \beta y^T y 
}

The integral will take care of the first term, leaving us with the inverse coefficient on the exponential for a multivariate gaussian:

\eq{
	\paren{ (2 \pi)^{-\frac{D}{2}} |\Sigma|^{\nhaf} }^{-1}
}

Therefore the terms we have left are

\eq{
	(2\pi)^{-\frac{D-D}{2}} |\alpha I|^{\haf} |\Sigma|^{\haf} \paren{\frac{\beta}{2\pi}}^{-\frac{N}{2}} \bexp{\nhaf \paren{\qf{(\beta Xy)}{ \Sigma} + \beta y^T y }}
}

So the log likelihood is

\eq{
	&-\frac{N}{2} \log (2\pi) + \frac{D}{2} \log \alpha - \haf \log |\Sigma| + \frac{N}{2} \log \beta - \haf \paren{\qf{(\beta Xy)}{ \Sigma} + \beta y^T y }
}

We can do some more work with the expression

\eq{
  & - \qf{(\beta Xy)}{ \Sigma} + \beta y^T y \\
  &= - \beta^2 y^T X^T \Sigma X y + \beta y^T y \\
  &= - \beta^2 y^T X^T \minv \Sigma \Sigma \minv \Sigma X y + \beta y^T y \\
  &= - \hat w ^T \Sigma \hat w + \beta y^T y \\
  &= - \hat w ^T \minv \Sigma \hat w + \beta y^T y \\
  &= - \alpha \hat w^T \hat w - \beta \hat w^T X X^T \hat w + \beta y^T y \\
  &= - \alpha \hat w^T \hat w +
  \beta \paren{y^Ty - \hat w^T X X^T \hat w} 
}

We will complete the square for the second term for $y$. Clearly, the quadratic term has coefficient $I$, and by looking at the constant term, we know $b^T I^{-1} b = \hat w^T XX^T \hat w$ so $b = X^T \hat w$. Therefore, we will need to inject the term $-2 b^T y$ into the expression.


\eq{
  &y^T y - 2\hat w^T X y - \hat w^T XX^T \hat w + 2 \hat w^T X y\\
  &= \sos{(y-X^T \hat w)} + \hat w^T XX^T \hat w  - 2\hat w^T XX^T \hat w + 2 \hat w^T X y\\
  &= \sos{(y-X^T \hat w)} -2 \hat w^T XX^T \hat w+ 2 \hat w^T X y
}

Let's start by eliminating the last two terms.

\eq{
  2 \hat w^T X y &= 2 \beta y^T X^T \Sigma X y \\
  &= \frac{2}{\beta} \hat w^T \minv \Sigma w
}

Canceling out with the $\beta$ coefficient on the whole second term, this gives us
\eq{
  2 \hat w^T \minv \Sigma \hat w
}

So we have collectively:

\eq{
  &= - \alpha \hat w^T \hat w + \beta \sos{(y-X^T \hat w)} + 2 \hat w^T \minv \Sigma \hat w \\
  &= - \alpha \hat w^T \hat w + \beta \sos{(y-X^T \hat w)} + 2 \alpha \hat w^T \hat w + 2 \beta \hat w^T XX^T \hat w \\
  &= \alpha \hat w^T \hat w + \beta \sos{(y-X^T \hat w)} + 2 \beta \hat w^T XX^T \hat w \\
}

If we pop back the last term inside the $\beta$ coefficient, we get $2\hat w^T XX^T \hat w$ which cancels out perfectly with the other term left over. This leaves us with

\eq{
  \alpha \hat w^T \hat w + \beta \sos{(y-X^T \hat w)}
}

In conclusion, the log likelihood is:

\eq{
	&-\frac{N}{2} \log (2\pi) + \frac{D}{2} \log \alpha - \haf \log |\minv \Sigma| + \frac{N}{2} \log \beta - \frac{\alpha}{2} \hat w^T \hat w + \frac{\beta}{2} \sos{(y-X^T \hat w)}
}

Recall from a long time ago that we are trying to maximize the log-likelihood with respect to the hyperparameters. Let's take the derivative with respect to $\alpha$ and $\beta$.

\eq{
	\frac{\partial p(y|\alpha, \beta)}{\partial \alpha} &= \frac{D}{2\alpha} - \haf \frac{\partial}{\partial \alpha} \log |\minv \Sigma| - \haf \sos{\hat w}
}

Recall the differential of a log determinant:

\eq{
	d( \log |\minv \Sigma|) &= \tr(\Sigma d(\minv \Sigma)) \\
    &= \tr(\Sigma d(\alpha I + \beta XX^T)) \\
    &= \tr(\Sigma d\alpha)
}

So the derivative comes out to 
\eq{
	\frac{\partial p(y|\alpha, \beta)}{\partial \alpha} &= \frac{D}{2\alpha} - \haf \tr(\Sigma) - \haf \sos{\hat w}
}

What is $\tr(\Sigma) = \tr((\alpha I + \beta XX^T)^{-1})$? Recall that it is the sum of the inverse eigenvalues of the matrix $\alpha I + \beta XX^T$, as shown in the matrix identities section. 

\eq{
	(\alpha I + \beta XX^T) v_i &= \lambda_i v_i \\
    \beta XX^T v_i &= (\lambda_i - \alpha) v_i
}

Therefore, if $E[\beta XX^T]$ denotes all the relevant eigenvalues, the trace of $\Sigma$ is $\sum_{E[\beta XX^T]} (\lambda_i + \alpha)^{-1}$

Setting the derivative to 0 we expect:

\eq{
	0 &= \frac{D}{2\alpha} - \haf \sum_i \frac{1}{\lambda_i + \alpha} - \haf \sos{\hat w} \\
    0 &= D - \sum_i \frac{\alpha}{\lambda_i + \alpha} - \alpha \sos{\hat w} \\
    &= \sum_i 1- \frac{\alpha}{\lambda_i + \alpha} - \alpha \sos{\hat w} \\
    &= \sum_i \frac{\lambda_i}{\lambda_i + \alpha} - \alpha \sos{\hat w} \\
}

Making use of the fixed point iteration algorithm, we can arrange this in the form:

\eq{
  \alpha &= \frac{\sum_i \frac{\lambda_i}{\lambda_i + \alpha}}{\sos{\hat w}}
}

Now we will try to do the same thing will $\beta$
\eq{
  \frac{\partial p(y|\alpha, \beta)}{\partial \beta} &= \nhaf \frac{\partial}{\partial \beta} \log |\minv \Sigma| + \frac{N}{2\beta} + \frac{\partial}{\partial \beta} \frac{\beta}{2} \sos{(y-X^T \hat w)}
}

First we will tackle the log determinant.

\eq{
	d( \log |\minv \Sigma|) &= \tr(\Sigma d(\minv \Sigma)) \\
    &= \tr(\Sigma d(\alpha I + \beta XX^T)) \\
    &= \tr(\Sigma d\beta XX^T)
}

So the derivative comes out to $\tr(\Sigma XX^T)$. Recall that the trace is equivalent to the sum of the eigenvalues. Denote the eigenvalues of $\Sigma XX^T$ as $\xi_i$. We will manipulate the matrix $\Sigma XX^T$ into another form and watch how the eigenvectors change

\eq{
	\xi_i &\leftrightarrow \Sigma XX^T \\
	\frac{1}{\xi_i} &\leftrightarrow (\Sigma XX^T)^{-1} \\
	\frac{1}{\xi_i} &\leftrightarrow (XX^T)^{-1} \minv \Sigma \\
	\frac{1}{\xi_i} &\leftrightarrow (XX^T)^{-1} (\alpha I + \beta XX^T) \\
	\frac{1}{\xi_i} &\leftrightarrow \alpha (XX^T)^{-1} + \beta I \\
	\frac{1}{\xi_i} - \beta &\leftrightarrow \alpha (XX^T)^{-1} \\
	\frac{1}{\alpha} \paren{ \frac{1}{\xi_i} - \beta } &\leftrightarrow (XX^T)^{-1} \\
	\frac{\alpha}{\frac{1}{\xi_i} - \beta} &\leftrightarrow XX^T \\
	\frac{\alpha \beta}{\frac{1}{\xi_i} - \beta} & \leftrightarrow \beta XX^T
}

So we have

\eq{
	\frac{\alpha \beta}{\frac{1}{\xi_i} - \beta} &= \lambda_i \\
	\frac{\alpha \beta}{\frac{1 - \beta \xi_i }{\xi_i}} &= \lambda_i \\
	\frac{\alpha \beta \xi_i}{1 - \beta \xi_i } &= \lambda_i \\
    \frac{1 - \beta \xi_i}{\alpha \beta \xi_i} &= \frac{1}{\lambda_i}\\
    \frac{1}{\alpha \beta \xi_i} - \frac{\beta \xi_i}{\alpha \beta \xi_i} &= \frac{1}{\lambda_i} \\
    \frac{1}{\alpha \beta \xi_i} - \frac{1}{\alpha} &= \frac{1}{\lambda_i} \\
    \frac{1}{\alpha \beta \xi_i} &= \frac{1}{\lambda_i} + \frac{1}{\alpha}\\
    \frac{1}{\xi_i} &= \alpha \beta \paren{\frac{1}{\lambda_i} + \frac{1}{\alpha}} \\
    \frac{1}{\xi_i} &= \alpha \beta \frac{\lambda_i + \alpha}{\lambda_i \alpha} \\    
    \frac{1}{\xi_i} &= \beta \frac{\lambda_i + \alpha}{\lambda_i} \\    
    \xi_i &= \frac{1}{\beta} \frac{\lambda_i}{\lambda_i + \alpha}
}

Substituting this back in, we retrieve the expression for the trace.

\eq{
	\tr(\Sigma XX^T) &= \frac{1}{\beta} \sum_i \frac{\lambda_i}{\lambda_i + \alpha} \\
}

\subsubsection{Multivariate Linear Regression *}

This is quite a bit more involved that the univariate version because our weight matrix $W$ is now an $n$ by $m$ matrix. We say that the vectorized form of $W$ is gaussian.

\subsection{Bayesian Logistic Regression}

In logistic regression, the setup is similar:
\eq{
	w &\sim \nml{0}{\alpha I} \\
    y|w &\sim \text{Bernoulli}(\sigma(w^T x))
}

Problematically, the likelihood is no longer gaussian, which poses difficulties for an analytical solution. We can no longer solve $p(w|y)$ as easily. Therefore, we will use the laplace approximation, described in a later section. First consider the log posterior distribution, dropping the constant terms:

\eq{
	\ln p(w|y) &= -\frac{\alpha}{2} w^Tw + \sum_i y_i \log \sigma(w^Tx) + (1-y_i) \log (1-\sigma(w^Tx)) + \text{const}
}

We need to take the first derivative in order to find the mode and the second derivative in order to find the hessian. But first for convenience we show some identites:
\eq{
	d \sigma(z) &= d (1+e^{-z})^{-1} \\
    &= -(1+e^{-z})^{-2} de^{-z} \\
    &= \frac{e^{-z}}{(1+e^{-z})^{2}} dz \\
    &= \sigma(z) (1-\sigma(z)) dz \\
	d \log \sigma(z) &= \frac{1}{\sigma(z)} d \sigma(z) \\
    &= (1-\sigma(z))dz \\
    d \log (1-\sigma(z)) &= \frac{1}{1-\sigma(z)} d (1-\sigma(z)) \\
    &= \frac{- \sigma(z) (1-\sigma(z))}{(1-\sigma(z))} dz\\
    &= - \sigma(z) dz
}

\eq{
	\grad_w \ln p(w|y) &= -\alpha w + \sum_i y_i (1-\sigma) x + (1-y_i) (-\sigma) x \\
    &= -\alpha w + \sum_i (y_i-\sigma(w^Tx_i)) x_i
}

Therefore, we see that MAP estimation for logistic regression is equivalent to weight penalized MLE estimation of parameters. Now we will continue to find the hessian.

\eq{
	\hess \ln p(w|x) &= \grad_w -\alpha w + \sum_i (y_i-\sigma(w^Tx_i)) x_i \\
    &= - \alpha I - \sum_i \grad_w \sigma(w^Tx) x \\
    &= - \alpha I - \sum_i \sigma_i (1-\sigma_i) x_i x_i^T \\
}

Then our approximation gives us the posterior distribution

\eq{
	w|y &\sim \nml{\hat w}{-\hess \ln p(w|y)}
}

Computing the predicative distribution for logistic regression is more challenging because we can no longer use the linear gaussian model as we did in linear regression. 

\eq{
	p(y'|w,y) &= \int p(y'|w)p(w|y) dw \\
    &= \int \sigma(w^T x')p(w|y) dw \\
}

Notice that all $w$ that are orthogonal to $x'$ will produce the same value for $\sigma(w^Tx')$ so we can rewrite the integral as two parts: the inner level collapses $w$ space into the probability density of $\sigma(w^Tx' = a)$ and an outer level that carries out the integral

\eq{
	&= \int \sigma(a) p(w^Tx' = a | y) da \\
	&= \int \sigma(a) \int \delta(a-w^Tx') p(w|y) dw da \\
}

As we have shown earlier, the inner integral results in a gaussian density. The mean is simply the transformed mean of $w|y$, so $\mu_a = \hat w^T x'$. The variance can be computed as follows:

\eq{
	\sigma_a^2 &= \int p(a) \paren{a - E[a]}^2 da \\
    &= \int q(w) \paren{w^Tx' - \hat w^T x'}^2 dw \\
    &= \int q(w) (x'^T (w-\hat w))^2 dw \\
    &= \int q(w) \qf{x'}{(w-\hat w)(w-\hat w)^T} dw \\
    &= \qf{x'}{\paren{\int q(w) (w-\hat w)(w-\hat w)^T dw}} \\
    &= \qf{x'}{\paren{\int q'(w) ww^T dw}} \\
    &= \qf{x'}{\Sigma_q}
}

Where we use $q$ to denote the posterior distribution on weights and $q'$ to denote a shifted version of $q$ which sets the mean to be 0 and keeps the same covariance.

Now we are left with this integral:

\eq{
	\int \sigma(a) \nml{\mu_a}{\sigma_a}(a) da
}

This is intractable, but we can approximate the sigmoid function with something called a probit function, and solve analytically. The details of this are not shown here.

\subsubsection{Laplace Approximation}

For the normal distribution, the negative hessian of the log density function happens to be the precision matrix. The mode of the density function happens to be the mean. The laplace approximation approximates other probability distributions as a multivariate gaussian by taking advantage of this fact. Given a density $p(x)$, we can set $\hat \Lambda = - \hess_x \log p(x)$ and $\hat \mu = \argmax_x p(x)$ and then construct a normal distribution accordingly. Although this approximation is not always great, it can be used in a relatively large range of scenarios. 

\subsection{Spline and Wavelet Bases *}
.
\subsection{Bayesian Neural Networks}

Similar to bayesian logistic regression, we make use of the laplace approximation in order to render the posterior distribution as gaussian. All this requires is a mode of the posterior distribution and also the hessian, which we can computing using finite differences or automatic differentiation. The mode of the posterior is simply whatever weights the training converges to, when using weight regularization (which is equivalent to a bayesian prior on the weights with covariance $\alpha^{-1} I$). 

Now we will ask how the predicative distribution can be calculated, which is nontrivial in a similar manner to bayesian logistic regression. We are trying to perform the integral:

\eq{
	p(y'|y) &= \int p(y'|w) p(w|y) dw
}

Where $w|y$ is gaussian. $y'|w$ is also gaussian, but not in a nice way. In linear models, $y'|w$ would be gaussian with mean $w^T x'$ but in a neural network, the mean may be something like $f(x',w)$ for some arbitrary function $f$. This means we can't use the results we previously derived for conditional gaussians. The solution is to approximate $f$ by some function $f'$ which is linear in $w$ around $x'$ and $\hat w$. This is done using taylor expansions in the standard way. After this is done, the predicative distribution is straightforward to derive.

\section{Approximate Inference *}

\subsection{Expectation Maximization *}
.
\subsection{Variational Inference *}
.
\subsubsection{Inference for FC Gaussian CRFS *}

A very popular class of CRFs are dense gaussian crfs, used for image processing. In this case, variables are arranged in an n by n grid, with given unary potentials and pairwise potentials between every pair of points in the grid.

\section{Sampling Algorithms}

\subsection{Importance Sampling}

We often estimate $E_{x \sim p(x)}[f(x)]$
with a single sample $f(x)$. The idea of importance sampling is to sample from another distribution with higher probability where $f(x)$ peaks, in order to reduce variance.

\eq{
  E_{x \sim p(x)}[f(x)] &= \int p(x) f(x) dx \\
  &= \int q(x) \frac{f(x)p(x)}{q(x)} dx \\
  &= E_q \sparen{\frac{f(x)p(x)}{q(x)}}
}

Often, computing the normalization constant for $p$ or $q$ can be difficult, so we only know what they are up to some constant. Call the unnormalized distributions $\tilde p$ and $\tilde q$. 

We can estimate $E_{x \sim p(x)}[f(x)]$ in two phases: Compute the unnormalized expectation, compute the normalization factor, and then divide. Note that while each estimate is unbiased, the combined estimate is biased. This is known as self-normalized importance sampling

\eq{
  E_{x \sim p(x)}[f(x)] &= \nmz \int \tilde p(x) f(x) dx \\
  Z &= \int \tilde p(x) dx \\
  E_{x \sim p(x)}[f(x)] &\approx \frac{\sum_i \tilde p(x_i) f(x_i)}{\sum_i \tilde p(x_i)}
}

Interestingly, note that we sometimes use self-normalized importnce sampling even when the normalized probability distribution is known to us. This is because the variance of the estimate will decrease. For example, consider a random variable which is 0 most of the time and 1, with probability 0.01. Suppose our sample size is 2. If our sample is $\{0,0\}$ or $\{1,1\}$, then the estimate of the mean will be 0 and 1 respectively. Yet, if the sample is $\{0,1\}$, then self-normalizing makes our estimate $0.01$ instead of $0.5$.

We can also combine both importance sampling with self-normalized sampling

\iffalse
\todo{check this section}

\eq{
  E_{x \sim p(x)}[f(x)] &= \int p(x) f(x) dx \\
  &= \int q(x) \frac{f(x)p(x)}{q(x)} dx \\  
  &= \int q(x) \frac{f(x) Z_p^{-1} \tilde p(x)}{Z_q^{-1} \tilde q(x)} dx \\
  &= \frac{Z_q}{Z_p} \int q(x) \frac{f(x) \tilde p(x)}{\tilde q(x)} dx \\
  &= \frac{Z_q}{Z_p} E_q \sparen{\frac{f(x) \tilde p(x)}{\tilde q(x)}} \\
  &= E_q \sparen{\frac{\tilde q(x)}{\tilde p(x)}} E_q \sparen{\frac{f(x) \tilde p(x)}{\tilde q(x)}} \\
}

Notice that this only requires the ability to compute the ratio of $p$ and $q$ up to some constant factor.
\fi

\subsection{Monte Carlo Markov Chain *}
.
\subsubsection{Metropolis Hastings *}
.
\subsubsection{Reversible Jump MCMC *}
.
\subsection{Gibbs Sampling *}
.
\section{Optimization}

\subsection{Lagrange Multipliers}

We want to solve the problem

\eq{
  \min f(x) \\
  \st g(x) = 0
}

We claim that at the optimal point, $\grad f(x)$ is parallel or antiparallel to $\grad g(x)$. Suppose this wasn't the case. Then we could decompose $\grad f(x)$ into a component parallel to $\grad g(x)$ and a component perpendicular to $\grad g(x)$. Call these components $d_1$ and $d_2$. 

\eq{
  x' &= x+\epsilon d_2\\
  f(x') &= f(x) + \epsilon \grad f(x)^T d_2 \\
}

If $\grad f ^T d_2$ is positive, then we make $\epsilon$ negative, and vice versa. This means we have found some $f(x') > f(x)$. Yet at the same time, since $d_2$ is perpendicular to $\grad g$, we know that $g(x') = g(x) = 0$. This is a contradiction, therefore $\grad f$ and $\grad g$ are parallel. We write $\grad f(x) = \lambda \grad g(x)$.

The lagrangian is defined as

\eq{
  \lag(x, \lambda) &= f(x) + \lambda g(x)
}

In order to solve our constrained optimization problem, we find $\grad_x \lag = 0$ and solve for both $\lambda$ and $x$. Example:

\eq{
  \intertext{Our objective is}
  f(x) &= \qf{x}{A} \\
  g(x) &= w^T x - k \\
  \intertext{Now we can write the lagrangian and take the gradient}
  \lag(x,\lambda) &= \qf{x}{A} + \lambda (w^T x -k ) \\
  \grad_x \lag &= 2Ax + \lambda w \\
  \intertext{The following must hold when the gradient is 0}
  x &= -\frac{\lambda}{2} \minv A w \\
  \intertext{Now we can bring in the constraint}
  w^T x -k &= -\frac{\lambda}{2} \qf{w}{\minv A} -k \\
  \intertext{Now solve for $\lambda$ and then $x$}
  \lambda &= \frac{-2k}{\qf{w}{\minv A}} \\
  x &= \frac{-k \minv A w}{\qf{w}{\minv A}} \\
  \intertext{Finally, plug it back into $f$}
  f(x) &= \qf{\paren{\frac{-k \minv A w}{\qf{w}{\minv A}}}}{A} \\
  &= \paren{\frac{k}{w^T \minv A w}}^2 w^T \minv A A \minv A w \\
  &= \paren{\frac{k}{w^T \minv A w}}^2 w^T \minv A w \\
  &= \frac{k^2}{\qf{w}{\minv A}}
}

As a sanity check, consider $A = I$ and $w = \vec 1$ and $k = 1$. Furthermore, consider this in two dimensions. So basically we are minimizing $x^2 + y^2$ on the line $y = x+1$. The best solution is at the point $(0.5, 0.5)$, when the cost is 0.5. The equations above work out exactly to this answer.

\subsection{Lagrangian Duality}

Before we talk about lagrangian duality, we'll review linear programming duality, because I understand that better.

Example linear program:

\eq{
  \min\ &3x + 4y + 5z \\
  \st &3x + y > 4 \\
  &z + 2y > 7 \\
}

Then we see that a lower bound to the optimal value is 4, and another lower bound is 14. In fact, we can try to get the highest lower bound by the following:

\eq{
  \max\ &4a + 7b\\
  \st &3a < 3\\
  &a + 2b < 4\\
  &b < 5\\
}

Variables in the primal correspond to constraints in the dual and vice versa. There are several nice properties of this primal-dual pattern.

First, the optimum of these two programs is the same. Second, variables which are non-zero correspond to constraints which are tight, this is known as complementary slackness. 

Now we will move onto lagrangian duality.

The primal takes the form

\eq{
  \min_x \max_{\lambda} \lag(x, \lambda)
}

Since $\lambda$ always makes the penalty as high as possible, this is pretty much equivalent to minimizing $f(x)$ with the constraint that $g(x) = 0$ (otherwise, the lagrangian would be infinite).

The dual takes the form

\eq{
  \max_{\lambda} \min_x \lag(x, \lambda)
}

When seeing these nested $\max$ and $\min$ functions, the best way to think about it is to go left to right, and treating each successive optimization as a turn. So for example, in the primal, the $x$ player moves first, and then the $\lambda$ player moves second. This is counterintuitive because following the nesting, it seems like the $\lambda$ player actuall moves first, not second. However, since $x$ is not bound during the maximization of $\lambda$, $\max_\lambda \lag(x, \lambda)$ really involves the $\lambda$ player considering every move of $x$ and then choosing the best $\lambda$ to counter it.

Here's a reason to expect the second player to always do better. Consider a zero-sum game-matrix containing row-player rewards  where each row and column contains both small and large numbers. Then no matter what the first player does, the second player will have a good response.

Therefore we would expect the following to be true, and it does match up with our expectations from linear primality and duality.

\eq{
  \min_x \max_{\lambda} \lag(x, \lambda) \geq \max_{\lambda} \min_x \lag(x, \lambda)
}

This is basically saying that for every possible $x^*$ and every possible $\lambda^*$ we have

\eq{
  \max_\lambda \lag(x^*, \lambda) \geq \min_x \lag(x, \lambda^*)
}

In particular, it must hold true for the $x^*$ which makes the LHS the smallest and the $\lambda^*$ which makes the RHS the largest. The key is to notice that the $x^*$ which makes the LHS the smallest does not necessarily make the RHS the smallest, and from that we can prove the inequality.

\eq{
  \max_\lambda \lag(x^*, \lambda) \geq \lag(x^*, \lambda^*) \geq \min_x \lag(x, \lambda^*)
}

Strong duality usually holds, which means the primal and the dual have the same optimal value, so we can convert back and forth between the problems we want to solve.

In practice, if we have a problem in the form

\eq{
  \min &f(x) \\
  \st\ &g(x) = 0
}

This is equivalent to
\eq{
  \min_x \max_\lambda f(x) + \lambda g(x)
}

But by duality we can rewrite this as

\eq{
  \max_\lambda \min_x f(x) + \lambda g(x)
}

Using our example problem from before this is

\eq{
  \max_\lambda \min_x \qf{x}{A} + \lambda (w^Tx -k)
}

From before, the minimization happens when $x = -\frac{\lambda}{2} \minv A w$. At this time, the lagrangian evaluates to

\eq{
  &\frac{\lambda^2}{4} \qf{w}{\minv A} - \frac{\lambda^2}{2} \qf{w}{\minv A} -\lambda k \\
  &= -\frac{\lambda^2}{4} \qf{w}{\minv A} - \lambda k\\
}

To maximize this with respect to $\lambda$ we can take the gradient wrt $\lambda$.

\eq{
  - \grad_\lambda \frac{\lambda^2}{4} \qf{w}{\minv A} + \lambda k &= \lambda \haf \qf{w}{\minv A} + k
  \intertext{We can set this quantity to 0}
  \lambda &= \frac{-2k}{\qf{w}{\minv A}}
}

We can now see that this is the same value of $\lambda$ derived without using duality, and therefore we have arrived at the same solution.

\subsection{KKT Conditions}

In lagrangian multipliers, we dealt with equality constraints in the form $g(x) = 0$. However, we can also deal with constraints such as $h(x) < 0$. We will use $\lambda$ for the mulitpliers on these inequality constraints and $\mu$ as the coefficients on the equality ones.

The lagrangian has not changed much from before:

\eq{
  \lag(x, \lambda, \mu) &= f(x) + \mu g(x) + \lambda h(x)
}

Ignoring $g$, we claim that $\grad f = \lambda \grad h$, which is a claim we justified for lagrangian multipliers. A similar argument holds here. There are two main cases. Either the optimal point is inside the feasible region, or it is on the edge of the feasible region. If it is inside the feasible region, then we can set $\lambda = 0$. If it is on the edge, then the previous argument holds.

Basically, we want to solve a problem with all the following constraints (these are known as the KKT Conditions). 

\enum{
\item $\grad_x \lag = 0$
\item $\mu \geq 0$
\item $\mu g = 0$
}

\subsection{SGD Variants *}
.
\subsection{Fixed Point Iteration}

Suppose you are trying to minimize $g(x)$, so you want to set $g'(x) = 0$. Unfortunately, it is not always trivial to solve for $x$. For example, consider

\eq{
  g'(x) &= \cos(x)^3 - \sqrt{x}
}

Instead, we hope to transform to expression to something like $f(x) = x$. This is usually much easier to do

\eq{
  0 &= \cos(x) - \sqrt{x} \\
  \sqrt{x} &= \cos(x) \\
  x &= \cos(x)^2
}

Here we have $f(x) = \cos(x)^2$

Now all that remains is an algorithm to find the fixed point. It turns out that starting out with an arbitrary guess and repeatedly applying $f$ will converge to the fixed point under certain conditions.

One sufficient requirement for convergence is a Lipschitz constant of less than 1. The Lipschitz constant of a function is the greatest value $K$ such that the magnitude of the slope of the line between any two points on the function is at most $K$. Note that a upper bound on the derivative of a function translates to an upper bound on the Lipschitz constant of a function but not vice versa. In any case, the Lipschitz constant of $\cos(x)^2$ is exactly 1, therefore we should be able to recover $x$.

\subsection{BADMM Algorithm *}
.
\section{Reinforcement Learning}

\subsection{Optimal Control}

\subsubsection{LQR}

Let us start with the simplest case:

\eq{
	c(x,u) &= \qf{x}{C_x} + \qf{u}{C_u} \\
    d(x,u) &= D_x x + D_u u
}

The total cost that we are trying to minimize:

\eq{
	J(x) &= \min_\bfu \sum_i c(x_i, u_i) \\
    &= \min_\bfu \sum_i \qf{x_i}{C_x} + \qf{u_i}{C_u}
}

To get a recursive relationship, we can also write $J_t$ to denote the cost from time $t$ up until $T$. 

\eq{
	J_t(x) &= \min_\bfu \sum_{i=t}^T c(x_i, u_i) \\
    &= \min_{u_t} c(x, u_t) + J_{t+1}(d(x, u_t)) \\
    &= \min_{u_t} c(x, u_t) + J_{t+1}(D_x x + D_u u_t)
}

We want to minimize $J = J_1$. We will develop an inductive solution which can minimize every $J_t$. We claim that $J_{t}(x)$ can be written as a quadratic function of $x$ for every $t$. 

In the base case, we have 
\eq{
	J_T(x) &= 0 \\
    &= \qf{x}{P_T}
    \intertext{where $P_T$ is a zero matrix}
}

Suppose we have written $J_{t+1}(x) = \qf{x}{P_{t+1}}$

\eq{
	J_t(x) &= \min_{u_t} c(x, u_t) + J_{t+1}(D_x x + D_u u_t) \\
    &= \min_{u_t} \qf{x}{C_x} + \qf{u_t}{C_u} + \qf{(D_x x + D_u u_t)}{P_{t+1}} \\
    &= \min_{u_t} \qf{x}{C_x} + \qf{u_t}{C_u} + \qf{x}{D_x^T P_{t+1} D_x} + \qf{u_t}{D_u^T P_{t+1} D_u} \\
    & \qquad \qquad + x^T D_x^T P_{t+1} D_u u_t + u^T D_u^T P_{t+1} D_x x \\
}

Taking the gradient wrt $u_t$ and setting it to 0 gives:
\eq{
	0 &= 2C_u u_t + 2D_u^T P_{t+1} D_u u_t + 2D_u^T P_{t+1}D_x x \\
    &= (C_u + D_u^T P_{t+1} D_u) u_t + D_u^T P_{t+1}D_x x \\
    (C_u + D_u^T P_{t+1} D_u) u_t &= D_u^T P_{t+1}D_x x \\
    u_t &= (C_u + D_u^T P_{t+1} D_u)^{-1} D_u^T P_{t+1}D_x x
}

Let $K$ be the product of the matrices on the RHS, so that $u_t = Kx$. Now we can substitute $Kx$ into our expression:

\eq{
	J_t(x) &= \qf{x}{C_x} + \qf{(Kx)}{C_u} + \qf{x}{D_x^T P_{t+1} D_x} + \qf{(Kx)}{D_u^T P_{t+1} D_u} \\
    & \qquad + x^T D_x^T P_{t+1} D_u K x + x^T K^T D_u^T P_{t+1} D_x x \\
    &= \qf{x}{(C_x + K^T C_u K + D_x^T P_{t+1} D_x + K^T D_u^T P_{t+1} D_u K \\
    & \qquad \qquad + D_x^T P_{t+1} D_u K + K^T D_u^T P_{t+1} D_x)} \\
    &= \qf{x}{P_t}
}

This completes the proof. Now we will consider the full case:

\eq{
	c(x,u) &= c + \vt{c_u}{c_x}^T \vt{u}{x} + \haf \qf{\vt{u}{x}}{\mt{\Cuu}{\Cux}{\Cxu}{\Cxx}} \\
    d(x,u) &= D_x x + D_u u + \vecd
}

Without loss of generality, we require the cost matrix to be symmetric, so $\Cxu = \Cux^T$ and both $\Cxx$ and $\Cuu$ are symmetric. Note that it is not reasonable to assume that $D_u$ and $D_x$ are symmetric. However, we will maintain inductively that $P$ is symmetric.

We will repeat the same proof strategy, defining:

\eq{
	J_T(x) &= \haf \qf{x}{P_T} + \vecpt x + p \\
    P_T &= \mathbf{0} \\
    \vecp &= \vec 0 \\
    p &= 0
}

Now we can expand the recurrence relation:

\eq{
	J_t(x) &= \min_{u} c(x, u) + J_{t+1}(D_x x + D_u u + \vecd) \\
    &= \min_{u} c + \cut u + \cxt x + \haf \qf{x}{\Cxx} + \haf \qf{u}{\Cuu} + x^T \Cxu u \\
    & \qquad \qquad + \haf \qf{(D_x x + D_u u + \vecd)}{P} \\
    & \qquad \qquad + \vecpt (D_x x + D_u u + \vecd) + p \\
    &= \min_{u} c + \cut u + \cxt x + \haf \qf{x}{\Cxx} + \haf \qf{u}{\Cuu} + x^T \Cxu u \\
    & \qquad \qquad + \haf \qf{x}{\Dxm^T P \Dxm} + \haf \qf{u}{\Dum^T P \Dum} + 
    \haf x^T D_x^T P D_u u + \haf u^T D_u^T P D_x x\\
    & \qquad \qquad + \vecdt P D_x x + \vecdt P D_u u + \vecdt \vecd
    + \vecpt D_x x + \vecpt D_u u + \vecpt \vecd + p \\
}

Taking the gradient wrt $u$ and setting it to 0 gives:

\eq{
	0 &= c_u + \Cuu u + \Cux x + D_u^T P D_u u + D_u^T P D_x x + D_u^T \vecp + D_u^T P \vecd\\
    &= (\Cuu + \Dum^T P \Dum) u + (\Cux + D_u^T P D_x) x + (c_u + D_u^T \vecp + D_u^T P \vecd) \\
    &= Q_u u + Q_x x + \vecq \\
    u &= - \minv Q_u Q_x x - \minv Q_u \vecq \\
    &= Kx + \veck
}

Now we will plug $u = (Kx+k)$ back into the objective:

\eq{
    J(x) &= c + \cut (Kx+\veck) + \cxt x \\
    & \qquad + \haf \qf{x}{\Cxx} + \haf \qf{(Kx+\veck)}{\Cuu} + x^T \Cxu (Kx+\veck) \\
    & \qquad + \haf \qf{(D_x x + D_u (Kx+\veck) + \vecd)}{P} \\
    & \qquad + \vecpt (D_x x + D_u (Kx+\veck) + \vecd) + p \\
\intertext{Denote $M = D_x + D_u K$ and $\vecm = D_u \veck = \vecd$}
	&= c + \cut (Kx+\veck) + \cxt x \\
    & \qquad + \haf \qf{x}{\Cxx} + \haf \qf{(K x+\veck)}{\Cuu} + x^T \Cxu (K x+\veck) \\
    & \qquad + \haf \qf{(M x + \vecm)}{P} + \vecpt (M x + \vecm) + p \\
\intertext{Expand}
	&= c + \cut K x + \cut \veck + \cxt x + \haf \qf{x}{\Cxx} + \haf \qf{\veck}{\Cuu} + \haf \qf{x}{K^T \Cuu K} \\
    & \qquad + x^T K^T \Cuu \veck + x^T \Cxu K x + x^T \Cxu \veck \\
    & \qquad + \haf \qf{\vecm}{P} + \haf \qf{x}{M^TPM} + x^T M^T P \vecm + \vecpt M x + \vecpt \vecm + p \\
\intertext{Now group the terms by their order}
	&= c + \cut \veck + \haf \qf{\veck}{\Cuu} + \haf \qf{\vecm}{P} + \vecpt \vecm + p \\
    & \qquad + \cxt x + \cut K x + x^T K^T \Cuu \veck + x^T \Cxu \veck + x^T M^T P \vecm + \vecpt M x \\
    & \qquad + \haf \qf{x}{\Cxx} + \haf \qf{x}{K^T \Cuu K} + x^T \Cxu K x + \haf \qf{x}{M^TPM}
\intertext{Regroup into quadratic form}
	&= c + \cut \veck + \haf \qf{\veck}{\Cuu} + \haf \qf{\vecm}{P} + \vecpt \vecm + p \\
    & \qquad + (c_x + K^T c_u + K^T \Cuu \veck + \Cxu \veck + M^T P \vecm + M^T \vecp)^T x \\
    & \qquad + \haf \qf{x}{(\Cxx + K^T \Cuu K + 2\Cxu K + M^TPM)}
}

This gives us the update rules:

\eq{
	P &\la \Cxx + K^T \Cuu K + 2\Cxu K + M^TPM \\
    \vecp &\la c_x + K^T c_u + K^T \Cuu \veck + \Cxu \veck + M^T P \vecm + M^T \vecp \\
    p &\la c + \cut \veck + \haf \qf{\veck}{\Cuu} + \haf \qf{\vecm}{P} + \vecpt \vecm + p
}

To summarize our expressions:

\eq{
	Q_u &= \Cuu + \Dum^T P \Dum \\
    Q_x &= \Cux + D_u^T P D_x \\
    \vecq &= c_u + D_u^T \vecp + D_u^T P \vecd \\
    K &= - \minv Q_u Q_x \\
    \veck &= - \minv Q_u \vecq \\
    M &= D_x + D_u K \\
    \vecm &= D_u \veck + \vecd
}

This concludes the proof.

\subsubsection{MPC and ILQR}

ILQR is an adaptation of LQR to nonlinear systems. It consists of successively rolling out a trajectory and using finite differences to linearize / quadratisize the dynamics and the cost function at each point in time, and then resolving for the optimal trajectory using the new dynamics and cost model. Each successive iteration of trajectory optimization should start from the previous iteration in order to help convergence. 

MPC runs ILQR and then takes the first action proposed in the trajectory. However, model predicative control is different in that ILQR should be re-run once for every time-step, allowing it to do well at stochastic scenarios. 

\subsubsection{Stochastic Optimal Control *}
.
\subsection{Q-Learning}

The bellman equations (I'm not sure if these are actually the bellman equations):

\eq{
Q(s,a) &= \sum_{s', r} P(s',r|s,a) \paren{r + \gamma V(s')} \\
&= \sum_{s', r} P(s',r|s,a) \paren{r + \gamma \sum_{a'} \pi(a'|s')Q(s',a')} \\
&= E_{s',r} \sparen{r + \gamma E_{a'} \sparen{ Q(s',a') } }
}

These expectations can be approximated by sampling, so we can define an error term $\delta$

\eq{
	\delta &= Q(s,a) - r - \gamma Q(s',a') 
}

Which should on average be 0. This is called the TD-error and we can train a neural network to minimize it.

\subsection{Policy Gradients}

The policy gradients algorithm tries to maximize the following expected reward under some policy $\pol$:

\eq{
  \min E_{\pol}[R(\tau)] 
}

We can take the gradient using a nice chain rule trick

\eq{
  \gradt E_{\pol}[R(\tau)] &= \gradt \int p_\theta(\tau) R(\tau) d \tau \\
  &= \int \grad p_\theta(\tau) R(\tau) d \tau \\
  &= \int p_\theta(\tau) \gradt \log p_\theta(\tau) R(\tau) d \tau \\
  &= E_{\pol}[\gradt \log p_\theta(\tau) R(\tau) ]
}

Where the gradient of the log probability can be rewritten by
\eq{
  \gradt \log p_\theta &= \gradt \log \prod_i p(\tau_{i+1}|\tau_i) \\
  &= \gradt \log \prod_i \sum_u p(\tau_{i+1}|u) \pol(u | \tau_i)\\
  &= \sum_i \gradt \log \sum_u p(\tau_{i+1}|u) \pol(u | \tau_i)\\
}

This has the disadvantage of requiring information about the dynamics of the universe, in addition to having to sum over all possible actions, which could be expensive, or intractable when $u$ is continuous.

Instead, we will prove the policy gradient theorem, which says

\eq{
	\gradt E_{\pol}[R(\tau)] &= \sum_s \mu_\pi(s) \sum_a q_{\pol}(s,a) \gradt \pol(a|s,\theta)
}

To begin, note that the expected reward of a trajectory is the value of the start state. If there is a distribution of start states, we can still carry on without loss of generality by introducing a new special start state which just goes to the other start states. But first, we will consider the value of an arbitrary state.

\eq{
	\gradt V_{\pol}(s) &= \gradt \sum_{a} \pol(a|s) q_{\pol}(s, a) \\
    &= \sum_{a} \gradt \pol(a|s) q_{\pol}(s, a)
    + \pol(a|s) \gradt \paren{R(s, a) + \gamma \sum_{s'} P(s'|a, s) V_{\pol}(s')}\\
    &= \sum_{a} \gradt \pol(a|s) q_{\pol}(s, a) 
    + \gamma \pol(a|s) \paren{\sum_{s'} P(s'|a, s) \gradt V_{\pol}(s')}\\
    &= \sum_{a} \gradt \pol(a|s) q_{\pol}(s, a) \gamma \pol(a|s) 
    \bigg( \sum_{s'} P(s'|a, s) 
    	\bigg( \sum_{a} \gradt \pol(a|s) q_{\pol}(s, a) \\
    		& \qquad \qquad \qquad \qquad + \gamma \pol(a'|s') (\cdots) 
        \bigg) 
    \bigg)
}

Notice that this recursion can be unrolled forever, and since the $\gamma$ term sets up an exponential decay, we can just ignore the final terms in the expression. All that is left is to sum of all the terms $\gradt \pol(a|s) q_{\pol}(s, a)$. The first term has the constant multiplier 1, the next term has the constant multiplier $\gamma P(s'|s)$, the third term has the constant $\gamma^2 P(s''|s)$ and so on. Let $\mu_{\pol}(s';s)$ denote the expected discounted visitation counts of state $s'$, starting from $s$. This expression is pretty easy to make use of in practice, because it can be sampled for by rolling out the policy. Then we can write simply:

\eq{
	\gradt V_{\pol}(s) &= \sum_s \mu_{\pol}(s';s) \sum_a \gradt \pol(a|s') q_{\pol}(s',a)
}

Now let's focus on the start state, Let $\mu_{\pol}(s) = \mu_{\pol}(s;s_0)$.

\eq{
	\gradt E_{\pol}[R(\tau)] &= \sum_s \mu_{\pol}(s) \sum_a \gradt \pol(a|s) q_{\pol}(s,a) \\
    &= \sum_s \mu_{\pol}(s) E_{a \sim \pol} \sparen{ \gradt \log (\pol(a|s)) q_{\pol}(s,a) } \\
    &= E_{s,t \sim \pol} \sparen{ \gamma^t E_{a \sim \pol} \sparen{ \gradt \log (\pol(a|s)) q_{\pol}(s,a) }}
}

Note that we have managed to eliminate the sums into expectations (this was not previously possible), which means we can perform it by sampling.

\subsubsection{Natural Policy Gradients}

In the previous section, we parameterized the policy with a neural network with weights. Although a neural network can express any function, we might find it rather strange that the parameterization of our policy determines the training -- a different network architecture would take a different path in parameter space and therefore a different path -- perhaps better or worse -- in policy space. 

The idea of natural PG is to move directly in policy space, therefore in principle, the network architecture would not matter. 

\todo{write this section}

\subsubsection{Counterfactual Multi-Agent Policy Gradients}

We want to play some cooperative multi-player game with imperfect information. How can the policy gradients framework be applied? One naive method might be Independent Actor Critic -- each player runs actor critic by itself, and comes up with its own policy. However, this doesn't work well because of the credit assignment problem. Since all agents are simultaneously updating their policies, an agent does not know whether or not the reward was due to their action or not.

COMA is a variant of actor-critic which relies on a centralized critic. The idea is that the critic is privy to all the actions and the information of all the agents, and therefore can provide better feedback. 

Recall the advantage function used in Independent Actor Critic:

\eq{
	A^a_{\pol}(\tau,u) &= Q^a_{\pol}(\tau,u) - \sum_{u'} \pol(u'|\tau) Q^a_{\pol}(\tau,u') 
}

The super-script $a$ is used to denote which agent is performing the action. Action has been renamed to $u$ in order to reduce overloading. Furthermore, since the setting has imperfect information, we use $\tau$ to denote the past and current trajectory through observation-state, as a single observation does not provide the markovian property anymore.

In COMA, the idea is to freeze the actions of all the other agents, so their stochasticity is ignored. Instead of asking ''If I had taken another action, what would have happened?'', COMA asks ''If everyone had taken the same action, and only my action changed, what would have happened?''

\eq{
	A^a_{\pol}(s,u) &= Q_{\pol}(s,\u) - \sum_{u^a} \pol(u^a|\tau^a) Q_{\pol}(s,\u^{-a} \cdot u^a) 
}

The problem here is with the $Q$ network, which operates on the joint action space. This is exponential in the number of agents, which is problematic, because it means the network must have an exponential number of outputs. To solve this, we can redesign the Q-network to take as input the actions of $n-1$ agents, and then output the $Q$ values given the last agent. 

\subsection{Actor-Critic}

The policy gradients method often struggles from the problem of high variance. Note that the gradient is in the direction of positive $q$, which means if all actions yield positive $q$, then the gradient will go all over the place. Of course, in expectation it is a small movement in one direction, but since we are computing the expectation by sampling, we get poor results. To counter this, one popular method is to select some baseline, and subtract it from $q$, so that the gradient is now

\eq{
	\gradt E_{\pol}[R(\tau)] &= \sum_s \mu_{\pol}(s) E_{a \sim \pol} \sparen{ \gradt \log (\pol(a|s)) \paren{q_{\pol}(s,a) - B_s }}
}

Note that the baseline can depend on the state, but it can obviously not depend on the action. One popular baseline is $V_{\pol}(s)$. One naive method of estimating the value baseline is to keep a moving average of the reward at each state, but this does not transfer to large state-spaces. Instead, a value network is often a preferred choice.

The advantage function $A_{\pol}$ is defined:

\eq{
	A_{\pol}(s,a) &= Q_{\pol}(s,a) - V_{\pol}(s) \\
	\gradt E_{\pol}[R(\tau)] &= \sum_s \mu_{\pol}(s) E_{a \sim \pol} \sparen{ \gradt \log (\pol(a|s)) A_{\pol} (s,a) }
}

This leads to the advantage actor-critic algorithm, where we use a q-network.

\enum{
	\item perform rollouts
	\item update weights according to policy gradients using the advantage network
    \item update the q-network, using traditional methods such as TD-error
    \item repeat
}

Note that a Q-network is sufficient to compute the advantage, given the policy, since we can expand the advantage as

\eq{
	A_{\pol}(s,a) &= Q_{\pol}(s,a) - V_{\pol}(s) \\
	&= Q_{\pol}(s,a) - \sum_{a'} \pol(a'|s) Q_{\pol}(s,a') \\
}

\subsection{Trust Region Policy Optimization *}
.
\subsection{Guided Policy Search *}
.
\subsection{End to End Deep Visuomotor Policies *}
.
\section{Neural Networks}

\subsection{Variational Autoencoders}

Variational autoencoders are a tool for generative modeling. In other words, we want to find a model for how data is generated from some latent space or random noise $z$. We want to maximize the quantity $p(x)$.

\eq{
  p(x) &= \int p(z) p(x|z) dz
}

The conditional probability $p(x|z)$ is frequently in the form

\eq{
  x|z \sim \nml{f_\theta(z)}{g_\theta(z)}
}

where $f_\theta$ and $g_\theta$ are neural networks. Now consider trying to maximize $p(x)$ with stochastic gradient descent. We approximate the expectation with sampling:

\eq{
  p(x) &\approx \avgsum_i p_\theta(x|z_i)
}

Intuitively, the problem is that the normal distribution must be very narrow, especially in high dimensional spaces, in order to have a high probability. That same narrowness also ensures that the variance of a sample based approach is very high.

The solution is to use some importance sampling based approach, where we sampling $z$ from an alternate distribution $q$. 

\eq{
  p(x) &\approx \avgsum_i \paren{p_\theta(x|z_i) \frac{p(z_i)}{q(z_i)}}
}

Indeed, it makes sense for $q$ to be different depending on $x$. 

\eq{
  p(x) &\approx \avgsum_i \paren{p_\theta(x|z_i) \frac{p(z_i)}{q(z_i|x)}}
}

More precisely, under the distribution $q$:

\eq{
  z | x &\sim \nml{\mu_\theta(x)}{\Sigma_\theta(x)}
}

Again, we parameterize the probability distribution with a neural network. Now the question remains: How can we train the encoder $q$ to approximate the true $p(z|x)$ in order to minimize variance? Let's write down the expression for $\kl{q(z|x)}{p(z|x)}$ and play around with it.

\eq{
  \kl{q||p(z|x)} &= E_q \sparen{ \log q(z) - \log p(z|x) } \\
  &= E_q \sparen{ \log q(z) - \log \frac{p(x|z)p(z)}{p(x)} } \\
  &= E_q \sparen{ \log q(z) - \log p(x|z) - \log p(z) + \log p(x) } \\
  &= \kl{q}{p(z)} - E_q \sparen{ \log p(x|z)} + \log p(x) \\
  \log p(x) - \kl{q}{p(z|x)} &= E_q \sparen{ \log p(x|z)} - \kl{q}{p(z)}
}

We want to maximize $\log p(x)$ and minimize $\kl{q}{p(z|x)}$, so this expression just happens to be very useful. We just need to maximize the RHS now.

Computing $\kl{q}{p(z)}$ is easy, because $p(z)$ has an analytical form (the standard gaussin), and $q$ has a nice parameterized form. Now consider the term $E_q[\log p(x|z)]$. 

Naive strategy: Sample $z \sim q$. Compute the gradient of $\log p(x|z)$ in order to approximate gradient of expectation. This is not ideal because the gradient will not backpropagate to the network $q$. Solution: use reparameterization trick.  

\subsection{Architectures }

This section briefly describes a few novel neural architectures.

\subsubsection{Neural Turing Machine *}
.
%https://arxiv.org/pdf/1410.5401.pdf
\subsubsection{Neural GPU}

A neural GPU is almost a fully convolutional network with a few twists. First, the convolutional network is applied $T$ times to itself, so that it acts in a recursive manner. Note that the output and input must have the same shape for this to work. The network itself is composed of $L$ layers, each of which is a convolutional GRU layer. Within each layer, the weights are the same. A convolutional GRU layer performs a convolution, and then inputs the result into a standard GRU unit. 

The inspiration for neural GPU came from cellular automata, which are similar to the NGPU in that operations are local but highly parallel and also iterate over time. From details in the paper, training of the NGPU was non-trivial and involved techniques such as curriculum learning, parameter sharing relaxation, hyperparameter search, and noise injection in the gradients.

The NGPU performed extremely well at binary addition and multiplication and seemed to have nearly perfect generalization ability on different number lengths.

\subsubsection{Grid LSTM}

The idea behind grid LSTM is to employ the ability of LSTMs to remember on more than just the time-dimension. For example, LSTM connections can also be performed along the depth-axis of a neural network in order to keep gradients flowing properly. 

A 3-

\section{Mathematical Background}

\subsection{Matrix Differentials}

First let's define a few functions to play around with.

\itmz{
\item $f : \RR^n \ra \RR$
\item $g : \RR^n \ra \RR^m$
}

Now for some definitions, using what I believe is known as the numerator notation for matrix calculus (as opposed to denominator).

\itmz{
\item $\grad f$ is a column-vector in $\RR^n$ 
\item $\jac f$ is a row-vector in $\RR^n$
\item $\hess f$ is a matrix of size $n$ by $n$
\item $\jac g$ is a matrix of size $m$ by $n$
}

\itmz{
\item $[\grad f]_i$ is $\parf{f}{x_i}$
\item $\jac f$ is $(\grad f)^T$
\item $[\hess f]_{ij}$ is $\parfsq{f}{x_i}{x_j}$
\item $[\jac g]_{ij}$ is $\parf{f_i}{x_j}$
}

In order to differentiate expressions, we will compute differentials instead. How do we go from computing differentials to differentiating?

If $d(f(X)) = AdX$ then $\parf{f}{X} = A$. Actually we can get a bit more lax than that, it turns out that $d(f(x)) = tr(AdX)$ implies $\parf{f}{X} = A$. 

Differentials are linear operators. Things work as you expect when dealing with sums, constants, and transposes. However, there are a few tricky cases.

The product rule works as usual, but order is very important, because multiplication does not commute anymore. As written below, the shapes should be compatible. 
\eq{
  d \paren{f(x)g(x)} &= df(x)g(x) + f(x)dg(x)
}

An example of the product rule in use.

\eq{
  d(x^Tx) &= (dx^T)x + x^T(dx) \\
  &= (dx)^Tx + x^T(dx) \\
  &= x^T(dx) + x^T(dx) \\
  &= 2x^T(dx)
}

The same warning about order applies to chainrule as well.
\eq{
  y &= g(x) \\
  d(f(g(x))) &= d_y(f(y)) d_x(g(x)) 
}

And the example:

\eq{
  d((x^Tx)^5) &= 5(x^Tx)^4 d(x^Tx) \\
  &= 10(x^Tx)^4 x^T(dx)
}

One very useful tool is the trace operator, written $tr$ which simply takes the sum of all the elements on the diagonal of the matrix. Trace is a linear operator and comes with all the corresponding convenient properties. 

Some other useful properties of trace.

\eq{
  tr(ABC) &= tr(CBA) & \text{cyclic permutation} \\
  tr(XX^T) &= ||X||^2 & \text{the frobenius norm}  \\
  tr(A) &= tr(A^T) \\
}

Example of using traces:

\eq{
  d(||Z-xx^T||^2) &= d tr(\paren{Z-xx^T}\paren{Z-xx^T}^T) \\
  &= d tr(ZZ^T + (xx^T)(xx^T)^T - Z(xx^T)^T - xx^TZ^T) \\
  &= d tr(ZZ^T + xx^Txx^T - Zxx^T - xx^TZ^T) \\
  &= d tr(ZZ^T) + d tr(xx^Txx^T) - d tr(Zxx^T) - d tr(xx^TZ^T) \\
  &= d tr(x^Txx^Tx) - d tr(Zxx^T) - d tr(Zxx^T) \\
  &= d (x^Tx)^2 - 2 d tr(Zxx^T) \\
  &= 4x^Tx x^T (dx) - 2 d tr(x^T Z x) \\
  &= 4x^Tx x^T (dx) - 2 d(x^T Z x) \\
  &= 4x^Tx x^T (dx) - 4 x^T Z dx\\
}

Note this assumes $Z$ is symmetric.

There are two special cases of differentials -- inverses, and log determinants. I won't prove these claims.

\eq{
  d(X^{-1}) &= -\minv{X}(dX)\minv{X} \\
  d(\log \mdet{X}) &= tr(\minv{X} dX)
}

\subsection{Calculus of Variations}

Normal calculus deals with scalar values and expressions. Multivariable and matrix calculus deals with vector and matrix values and expressions. Calculus of variances deals with values and expressions which are functions. This is very applicable because probability distributions are a very commonly used object which are functions, and we often want to maximize objectives defined on probability distributions.

In functional calulus, we deal with functionals, which often take the following form, where $G$ is some arbitrary function.

\eq{
F[f] &= \int_a^b G(f(x)) dx
}

In normal calculus, we often make use of taylor expansions like this.

\eq{
f(x+\epsilon) &= f(x) + \epsilon \frac{df(x)}{dx} + O(\epsilon^2)
}

We can extend the idea of taylor expansions to functionals. Instead of adding or subtracting $\epsilon$, we add $\epsilon \eta(x)$, where $\eta$ is an arbitrary function. Everything stays about the same, except with an additional integral. 

\eq{
F[f+\epsilon \eta] &= F[f] + \epsilon \int_a^b \frac{\partial F[f]}{\partial f(x)}\eta(x) dx + O(\epsilon^2)
}

In order to find a stationary point in single variable calculus, we set $\frac{df(x)}{dx} = 0$, and the same holds in the variational case.

\eq{
0 &= \int_a^b \frac{\partial F[f]}{\partial f(x)}\eta(x) dx
}

Since this has to hold for arbitrary $\eta$, it must be the case that $\frac{\partial F[f]}{f(x)}$ is 0 everywhere. Now we have a simple procedure for finding stationary points.

\enum{
\item Write down the linear term (the integral)
\item Factor out $\eta(x)$
\item Set the integrand to 0
\item Solve for $f$
}


We can start with the simple exercise of finding the function that most closely matches the $\sin$ function. Our functional is defined:

\eq{
  F[f] &= \int_0^1 (f(x)-\sin(x))^2 dx
}

Now we will rewrite the integral dropping out higher order terms in the process.

\eq{
  F[f+\epsilon \eta] &= \int_0^1 \paren{f(x) + \epsilon \eta(x) - \sin(x)}^2 dx \\
  &\approx \int_0^1 (f(x)-\sin(x))^2 + \epsilon \eta(x) (f(x)-\sin(x)) dx \\
  &= F[f] + \epsilon \int_0^1 \paren{f(x)-\sin(x)} \eta(x) dx
}

Therefore the term we want to set to 0 is $f(x)-\sin(x)$, means $f(x) = \sin(x)$, exactly as expected.

\subsubsection{Euler-Lagrange Equation}

The Euler-Lagrange equation is a functional optimization problem which comes up everywhere, so we might as well solve it now. The lagrangian (not to be confused with the other uses), is some function $G(x, y, y')$, where $y = f(x)$. The functional takes the form:

\eq{
  F[f] &= \int_a^b G(x, y, y') dx
}

Furthermore, this comes with the restriction that $f$ is fixed at both endpoints $a$ and $b$, which means $\eta(a) = \eta(b) = 0$. Why this is useful will become apparent later.

The taylor-expansion must therefore involve terms in both $y$ and $y'$:

\eq{
  F[f+\epsilon \eta] &= F[f] + \epsilon \int_a^b \paren{\frac{\partial G}{\partial f(x)}\eta(x) + \frac{\partial G}{\partial f'(x)}\eta'(x)} dx + O(\epsilon^2)
}

We want to factor $\eta$ out of the integrand, so we can use our usual technique. In order to do this, we'll integrate the second term. Let's do this integration by parts. Quick review:

\eq{
  \int_a^b u dv = uv \big\rvert_a^b - \int_a^b vdu
}

We select $u = \frac{\partial G}{\partial f'(x)}$ and $dv = \eta'(x) dx$, which means

\eq{
  du &= \frac{d}{dx} \frac{\partial G}{\partial f'(x)} dx\\
  v &= \eta(x)
}

\eq{
  \int_a^b \frac{\partial G}{\partial f'(x)} \eta'(x) dx &=
  \frac{\partial G}{\partial f'(x)} \eta(x) \big\vert_a^b - \int_a^b \eta(x) \frac{d}{dx} \frac{\partial G}{\partial f'(x)} dx \\
}

Claim: the first term on the RHS goes to zero, because $\eta(a)$ and $\eta(b)$ are both 0. Therefore we are left with the second term, which we combine with the first term from before, to obtain the integral:

\eq{
  \int_a^b \paren{\frac{\partial G}{\partial f(x)} - \frac{d}{dx} \frac{\partial G}{\partial f'(x)}}\eta(x) dx
}

This means that

\eq{
  \frac{\partial G}{\partial f(x)} &= \frac{d}{dx} \frac{G}{\partial f'(x)}
}

This completes the solution. Now we will demonstrate an application.

\subsubsection{Shortest Path Problem}

Let's find the shortest path from between two points, namely $(0,0)$ and $(1,1)$. (Now it becomes clear why we said the endpoints of $f$ were fixed!).

Our functional is in the form

\eq{
  \int_0^1 \sqrt{1+\paren{\frac{df(x)}{dx}}^2} dx
}

We can use the euler-lagrange equation to solve this by setting:

\eq{
  G(x, f(x), f'(x)) &= \sqrt{1+\paren{\frac{df(x)}{dx}}^2} 
}

Now we know that
\eq{
  \frac{\partial G}{\partial f} &= 0 \\
  \frac{\partial G}{\partial f'} &= \haf \paren{1+f'(x)^2}^{\nhaf} \frac{\partial}{\partial f'} \paren{1+\paren{\frac{df(x)}{dx}}^2} \\
  &= \frac{f'}{\sqrt{1+f'(x)^2}}
}

So according to our solution, we have:
\eq{
  0 &= \frac{d}{dx} \frac{f(x)'}{\sqrt{1+f'(x)^2}} \\
  C &= \frac{f(x)'}{\sqrt{1+f'(x)^2}}
}

For some constant $C$ and for all $x$. Now we will solve for $f'(x)$, since given $f(0) = 0$ and $f'(x)$ for all $x$, we can recover the function $f$.

\eq{
  C &= \frac{f(x)'}{\sqrt{1+f'(x)^2}} \\
  C^2 &= \frac{f'^2}{1+f'^2} \\
  C^2(1+f'^2) &= f'^2 \\
  0 &= f'^2(C^2-1) + C^2 \\
  f'^2 &= \frac{C^2}{1-C^2} \\
  f' &= \sqrt{\frac{C^2}{1-C^2}}
}

Now we know that $f'$ is constant, which means we have drawn a straight line from $(0,0)$ to $(1,1)$. This completes the proof.

\subsubsection{Maximum Entropy Distrbutions}

As an exercise, let us prove that the gaussian distribution has maximum entropy under some constraints (zero mean and unit variance). 

Formally, 
\eq{
  \max H[x] \\
  \text{such that}
  \int f(x) dx &= 1 \\
  \int x f(x) dx &= 0 \\
  \int x^2 f(x) dx &= 1 \\
}

Writing this as a lagrangian, we want to solve
\eq{
  L(x,\lambda) = &-\int f(x) \log f(x) dx + \\
  &\left( \lambda_1 \int f(x) dx -1 \right) + \\
  &\left( \lambda_2 \int xf(x) dx \right) + \\
  &\left( \lambda_3 \int x^2 f(x) dx -1 \right) \\
  \lambda &\neq 0\\
  &\nabla_x L(x,\lambda) = 0
}

First we will take the functional derivative of each term in the lagrangian.
\eq{
  &-\lim_{\epsilon \rightarrow 0} \frac{\partial}{\partial \epsilon} \int (f(x) + \epsilon \eta(x)) \log(f(x) + \epsilon \eta(x)) dx \\
  &= -\lim_{\epsilon \rightarrow 0} \int \eta(x) \log(f(x) + \epsilon \eta(x)) + (f(x) + \epsilon \eta(x) \frac{\partial}{\partial \epsilon} \log(f(x) + \epsilon \eta(x)) dx \\
  &= -\lim_{\epsilon \rightarrow 0} \int \eta(x) \log(f(x) + \epsilon \eta(x)) + (f(x) + \epsilon \eta(x) \frac{\eta(x)}{f(x) + \epsilon \eta(x)} dx \\
  &= -\lim_{\epsilon \rightarrow 0} \int \eta(x) \log(f(x) + \epsilon \eta(x)) + \eta(x) dx \\
  &= -\lim_{\epsilon \rightarrow 0} \int \eta(x) \left( 1+ \log(f(x) + \epsilon \eta(x))\right)  dx \\
  &= -\eta(x) \int 1+\log f(x) dx
  \intertext{Therefore}
  \frac{\partial H[x]}{\partial f(x)} &= -1-\log f(x)
}

Now for the normalization constraint:
\eq{
  &\lim_{\epsilon \rightarrow 0} \frac{\partial}{\partial \epsilon} \int f(x) + \epsilon \eta(x) dx\\
  &= \lim_{\epsilon \rightarrow 0} \int \eta(x) dx
  \intertext{Therefore the derivative is 1}
}

Next, we have the zero mean constraint:
\eq{
  &\lim_{\epsilon \rightarrow 0} \frac{\partial}{\partial \epsilon} \int x(f(x) + \epsilon \eta(x)) dx\\
  &= \lim_{\epsilon \rightarrow 0} \frac{\partial}{\partial \epsilon} \int \epsilon x\eta(x)) dx\\
  &= \lim_{\epsilon \rightarrow 0} \int x\eta(x) dx
  \intertext{Therefore the derivative is $x$}
}

Finally for the unit variance constraint:
Next, we have the zero mean constraint:
\eq{
  &\lim_{\epsilon \rightarrow 0} \frac{\partial}{\partial \epsilon} \int x^2(f(x) + \epsilon \eta(x)) dx\\
  &= \lim_{\epsilon \rightarrow 0} \frac{\partial}{\partial \epsilon} \int \epsilon x^2\eta(x)) dx\\
  &= \lim_{\epsilon \rightarrow 0} \int x^2\eta(x) dx
  \intertext{Therefore the derivative is $x^2$}
}

We may now find the gradient of the lagrangian and set it to 0.
\eq{
  \frac{\partial L}{\partial x} &= -1-\log f(x) + \lambda_1(1) + \lambda_2x + \lambda_3x^2 = 0\\
  \log f(x) &= -1+\lambda_1 + \lambda_2x + \lambda_3x^2 \\
  f(x) &= \exp \left( -1+\lambda_1 + \lambda_2x + \lambda_3x^2 \right)
}

Now it remains to solve for the $\lambda$ terms. By inspection, $\lambda_2 = 0$. It turns out that $\lambda_3 = -\frac{1}{2}$ and $\lambda_1 = 1-\frac{\log 2 \pi}{2}$. Therefore our distribution is the standard gaussian.

We can generalize this to prove that a gaussian has the maximum entropy for all means and variances. Note that the mean has no effect on entropy, and scaling by some constant $k$ will increase entropy by $\log k$ and the variance by $k^2$.

If the standard gaussian has entropy $h$, then the gaussian with variance $\sigma^2$ will have scale factor $\sigma$ and hence entropy will be $h+\log k$. But if there is another non-gaussian distribution with entropy $h' > h+\log k$, then it can be scaled by $\sigma^{-1}$ to obtain a distribution with unit variance and entropy $h' - \log k > h$. Since this is impossible, we know there is no non-gaussian distribution of any variance and mean which has maximum entropy.

\section{Miscellaneous}

\subsection{Echo State Networks}

Echo state machines are relic of the past, from when RNNs could not be trained effectively. The main component of an ESN is a reservoir of neurons which are sparsely connected together. The input connects to the reservoir and the reservoir connects to the output. In addition, it is possible for output neurons to feed back into the reservoir, although this can cause stability issues.

Therefore, naive operation of the ESN may be expressed by the following expression, where $o$ is used to denote output neurons, $x$ is input neurons, and $h$ is the reservoir state.

\eq{
  h_t &= Ax + Bh_{t-1} \\
  o_t &= Ch_t
}

However, in order to give an ESN sufficiently long term memory, the reservoir should be damped with a damping factor $\lambda$. The modified equations for updating the reservoir with damping are

\eq{
  \hat{h_t} &= Ax + Bh_{t-1} \\
  h_t &= (1-\lambda)h_{t-1} + \lambda \hat{h_t}
}

The reservoir is initialized with random weights. There are guidelines on weight initialization, which we will not cover. Interestingly, reservoir weights are not trained at all.

Instead, only output weights $C$ are trained using standard backpropagation so that the outputs $o$ match the target sequence $y$. 

\subsection{Gumbel Softmax Trick}

Sampling from a gaussian distribution is differentiable using the reparameterization trick. In order to sample from $\nml{f(x)}{\sigma}$, where $f$ is an arbitrary function, we can simply sample from $\nml{0}{\sigma}$ and then add that noise value onto $f(x)$. Then when we backpropagate, the gradients run through $f(x)$ as well.

However, this doesn't seem to work for sampling from multinomial distributions. The gumbel softmax trick is a way to backpropagate from sampling from these distributions. 

The gumbel trick to sample from multinomial distributions goes like this. Given the vector $x$ whose components sum to 1, we compute $\log(x)$, and then we add noise from some distribution so that the chance that the $j$th component is largest is $p_j$. So sampling from $x$ is a matter of adding noise and taking $\argmax$. As it happens, the distribution which works is known as the gumbel distribution.

You might notice that Gumbel's trick still doesn't solve our problem, because the argmax we used kills the gradients. 

Commonly, when reparameterization does not work, we resort to straight-through estimators, where we replace the gradient of some non-differentiable function f(x) with a proxy g'(x). For example, $g(x) = x$ is a good approximation to the $\samp(x)$ function. Therefore, when we backpropagate, the gradient is multiplied by 1.

This approach is fairly effective, but it suffers from some deficiencies, namely high variance, since we are sampling.

In the gumbel softmax trick, we sample from $x$ by doing $\samp(\smx(\log(x)+g))$ where $g$ is something known as gumbel noise. Then when we backpropagate, replace the derivative of $\samp$ with $1$ as usual. Why does this have any lower variance than before? We might expect it to have higher variance, since we are both sampling and also adding noise. However, note that the sampling algorithm from before, which was just $\samp(x)$ is equivalent to $\samp(\argmax(\log(x)+g))$. By replacing $\smx$ with $\argmax$, we are clearly reducing variance, at the cost of some bias.

To gain finer control over the balance between bias and variance, we can replace the $\smx$ function with $\smx_\tau(x) = \smx\paren{x/\tau}$. When $\tau = 1$, this is equivalent to softmax. When $\tau = 0$, this approaches $\argmax$. Finally, when $\tau \rightarrow \infty$, this approaches the uniform distribution.

The gumbel softmax paper found good results by starting $\tau$ at a high value, perhaps 10, and then annealing it to a low value, like 0.1. It also made the suggestion that $\tau$ could even be a learned parameter. 

Now what is Gumbel's distribution and where did it come from? The best way to explain it is that a gumbel distribution is like the normal distribution but replacing the sum with the max. Just as the sum of sufficiently many iid random variables will converge to the normal, taking the max of sufficiently many iid random variables will converge to the gumbel distribution -- well almost. We need the additional assumption that the distributions have an exponentially decaying tail, which is true for many distributions, including everything in the exponential family.

The distribution itself has the following CDF and PDF:

\eq{
  F(x) &= \bexp{-\bexp{-x}} \\
  f(x) &= \nexp{x+e^{-x}}
}

The final piece of this puzzle is to prove that the gumbel trick actually works. Being lazy, I will only bother proving it in the binary case. 

\eq{
  &P(\log p_1 + g_1 > \log p_2 + g_2) \\
  &= P \paren{\log \frac{p_1}{p_2} + g_1 > g_2} \\
  &= \int f(x) P\paren{\log \frac{p_1}{p_2} + x > g_2} dx \\
  &= \int f(x) F\paren{\log \frac{p_1}{p_2} + x} dx \\
  &= \int \nexp{x+e^{-x}} \bexp{-\nexp{\log \frac{p_1}{p_2} + x}} dx \\
  &= \int \nexp{x+e^{-x} + \nexp{x + \log \frac{p_2}{p_1}}} dx \\  
  &= \int \nexp{x+e^{-x} + e^{-x} \bexp{\log \frac{p_2}{p_1}}} dx \\
  &= \int \nexp{x + \paren{1+\frac{p_2}{p_1}}e^{-x}} dx \\
  &= \int \nexp{x + ke^{-x}} dx \\
  &= \frac{1}{k} \\
  &= \frac{1}{\frac{p_1+p_2}{p_1}}\\
  &= \frac{p_1}{p_1+p_2} \\
  &= \frac{p_1}{p_1+p_2}
}

Proof of the integral which I skimmed over:

\eq{
  u &= e^{-x} \\
  x &= -\log u \\
  dx &= -\frac{1}{u} du \\
  &\int \nexp{x + ke^{-x}} dx \\
  &= \int \frac{1}{u} \nexp{-\log u + ku} du\\
  &= \int \frac{1}{u} \bexp{\log u - ku} du\\
  &= \int \frac{1}{u} u \nexp{ku} du\\
  &= \int \exp(-ku) du \\
  &= \frac{1}{k} exp(-ku) \bigm|_{-\infty}^{\infty} \\
  &= \frac{1}{k}
}

This completes the proof.

\subsection{Support Vector Machines}

The task of a support vector machine is to find a separating hyperplane which maximizes the distance of any point from the plane. How can we find the distance of a point $p$ from a plane parameterized by some normal vector $v$, such that all points $x$ on the plane satisfy $v^T(x-p_0) = 0$?. The distance of $p$ from the plane is the magnitude of the projection of $x-p_0$ onto $v$, which is $v^T(x-p_0$ whenever $v$ is a unit vector.

So the objective of SVM is to find some vector $v$ which maximizes the margin $\delta$ under the following constraints:

\itmz{
\item $v$ is a unit vector
\item $v^T(x_i-p_0) > \delta$ for all positive samples
\item $v^T(x_i-p_0) < -\delta$ for all negative samples
}

Writing $y_i$ for the label, we can rewrite both of the margin constraints as $y_i v^T(x_i -p_0) > \delta$. But we can drop the constraint on $v$ being unit by scaling this inequality appropriately

\eq{
  \frac{y_i v^T(x_i-p_0)}{||v||\delta} > 1
}

It is then apparent that we can replace the objective $\max \delta$ with $\min ||v||$.

A more intuitive way to see this is to view magnitude of $v$ as a factor which stretches or shrinks space. When $v$ is large, space is compressed, so the margins are small and moving a small distance away from the separating hyperplane is sufficient to satisfy the constraints. On the other hand, when $v$ is small, space is stretched out, and the margins are much larger.

A quick foray into geometry:
The traditional parameterization of the points at a certain distance $\delta$ from a plane with normal vector $v$ is
\eq{
\paren{\frac{v}{||v||}}^T (x-p_0) = \delta
}

Notice that we are using $2n+1$ variables to specify a plane which should only need $n$. This is because we normalize $v$ for convenience of reasoning. However, we can reparameterize planes and the constraint as:

\eq{
v^T x = 1
}
This is true by setting $||v|| = \frac{1}{\delta}$

\iffalse
This can be derived by using the magnitude of $v$ to indicate the offset of the plane. By setting

\eq{
  ||v|| &= \frac{1-v^Tp_0}{\delta} \\
  \delta &= \frac{1-v^Tp_0}{||v||}
  \intertext{We can plug this back into the original expression to get}
  \frac{1}{||v||}v^T(x-p_0) &= \frac{1-v^Tp_0}{||v||}\\
  v^Tx - v^Tp_0 &= 1-v^Tp_0 \\
  v^Tx &= 1
}
\fi

Also, in this form, it is immediately clear that the objective of SVM is to minimize $||v||$ with the constraint $y_i v^T(x-p_0) > 1$. Since this is a quadratic program (the objective is quadratic and the constraints are linear), we know this can be effectively solved.

The constraint can be further simplified by observing that $p_0$, which has $n$ degrees of freedom, can be replaced by $b = v^Tp_0$, so that our constraint can be written $y_i (v^Tx - b) > 1$. Now we can derive a more elegant solution using lagrangian duality. 

\eq{
  \lag(v, \lambda) &= \haf v^Tv + \sum_i \lambda_i (1-y_i (v^Tx_i - b)) \\
  \intertext{Now we will do the inner minimization wrt $v$}
  \grad_v \lag &= v - \sum_i \lambda_i y_i x_i \\
  v &= \sum_i \lambda_i y_i x_i
  \intertext{And the same for $b$}
  \grad_b \lag &= \sum_i \lambda_i y_i \\
  0 &= \sum_i \lambda_i y_i 
  \intertext{Plug this back in to the lagrangian}
  \lag(v, \lambda) &= \haf \paren{\sum_i \lambda_i y_i x_i}^T \paren{\sum_i \lambda_i y_i x_i} +
  \sum_i \lambda_i \paren{1-y_i \paren{ \paren{\sum_j \lambda_j y_j x_j}^T x_i -b}} \\
  &= \haf \sum_{i,j} \lambda_i \lambda_j y_i y_j x_i^T x_j +
  \sum_i \lambda_i - b\sum_i y_i - \sum_{i,j} \lambda_i \lambda_j y_i y_j x_i^T x_j \\
  &= \sum_i \lambda_i - b\sum_i y_i - \haf \sum_{i,j} \lambda_i \lambda_j y_i y_j x_i^T x_j 
}

Now our optimization problem is in the form
\eq{
  \max_\lambda & \sum_i \lambda_i - \haf \sum_{i,j} \lambda_i \lambda_j y_i y_j x_i^T x_j \\
  \st\ &\sum_i \lambda_i y_i = 0\\
  &\lambda_i \geq 0
}

This expression can now be optimized using your favorite convex optimization problem. One simple algorithm which works well is dual coordinate ascent. Simply select two $\lambda_i$ and $\lambda_j$ and line search greedily for a better solution. Repeat. When finished, we can recover an expression for $v$.

We can recover $b$ by computing the expression $v^Tx_i$ for all positive samples. Since we know we have satisfied the constraint $v^Tx_i - b > 1$, and in fact, $min(v^Tx_i -b) = 1$ when the constraint is tight, therefore, $min(v^Tx_i)-1 = b$. 

\subsubsection{Kernel Trick}

The most interesting thing about this dual form is that none of the vectors $x_i$ must be explicitly computed. Instead, it is sufficient that the dot product of $x_i^T x_j$ be computed. In fact, we can transform the inputs arbitrarily, and compute $K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$ even if computing $\phi$ itself is computationally expensive. The traditional euclidean kernel is $K(x_i, x_j) = x_i^Tx_j$. We can also use the gaussian kernel

\eq{
  K_\sigma(x_i, x_j) &= \bexp{-\frac{||x_i-x_j||^2}{2\sigma^2}}
}

The $\phi$ corresponding to this kernel is infinite-dimensional, so it is luckily that we can avoid computing it. In general, the only restriction on $K$ is that it must be semi-definite positive.

\subsubsection{Soft Margins}

We can also apply support vector machines to non linearly separable datasets by allowing points to be within the margin, or even on the otherside of the hyperplane, with some penalty. With this slack, the constraints are now

\eq{
  y_i (v^Tx_i -b) &> 1-\epsilon_i \\
  \epsilon_i \geq 0
}

We can add on a penalty term $C\sum_i \epsilon_i$ to the objective. This produces the lagrangian:

\eq{
  \lag(v, \lambda) &= \haf v^Tv + C\sum_i \epsilon_i +
  \sum_i \lambda_i (1-\epsilon_i - y_i (v^Tx_i - b)) \\
  &= \haf v^Tv + \sum_i (C-\lambda_i)\epsilon_i +
  \sum_i \lambda_i (1- y_i (v^Tx_i - b)) \\  
}

When minimizing with respect to $\epsilon$, it is clear that as long as $C-\lambda_i > 0$, we will pick $\epsilon_i = 0$. On the other hand, if $C -\lambda_i < 0$, then $\epsilon \rightarrow \infty$ and the lagrangian approaches $- \infty$. Therefore, the maximizer over $\lambda$ in the dual problem will set $0 < \lambda_i < C$ as an additional constraint.

\subsection{Gaussian Processes *}
.
\subsection{Independent Component Analysis}

In ICA, we have some random variables $s$ and some square mixing matrix $M$, and the observed variables are $x = Ms$. We want to find $x$. In order to do this, we assume $s$ is drawn from the logistic distribution, which is the distribution with the cumulative distribution function $\sigma(x)$. Furthermore, we assume each of the source variables is independent. We want to learn the unmixing matrix $U = M^{-1}$. 

Let's try to solve this with maximum likelihood.

\eq{
  p(s) &= p(Ux) \left| U \right| \\
  &= |U| \prod_{u \in U} \sigma'(u^Tx) \\
  \log p(s) &= \log |U| + \sum_{u \in U} \log \sigma'(u^T x)
}

At this point you just run gradient descent to get the answer. 

Another ICA algorithm involves kurtosis, which is a measure of nongaussianity. Kurtosis is the expected value of $x^4$. One may ask why we do not use skewness, which is the expectation of $x^3$. I'm not sure myself, although I have read vague and unsubstantiated claims to the effect that kurtosis somehow encompasses skewness through something called a one-bit-matching theorem.
The algorithm performs kurtosis maximization in order to find the directions along which the data is projected into the least gaussian shape possible, but that's all I know about it.

\subsection{Class Imbalance}

One simple way to correct class inbalance is by training a classifier with equal amounts of data from each class. For example, we might take only a subset of the larger class, or bootstrap by sampling multiple times from the smaller class.

To be precise, suppose a classifier is trained on an artificially balanced dataset with $k$ classes. Therefore we have obtained some probability distribution $q(c|x)$. How can we recover the true distribution $p(c|x)$?

First, we use Bayes' Theorem to obtain
\eq{
q(c|x) &= \frac{q(x|c)q(c)}{q(x} \propto q(x|c)q(c) \\
p(c|x) &= \frac{p(x|c)p(c)}{p(x} \propto p(x|c)p(c) \\s
\intertext{Furthermore notice that $p(x|c) = q(x|c)$, due to the fact that within a single class, we did not manipulate the data}
p(c|x) &\propto q(x|c)p(c) \\
&= q(x|c)q(c) \frac{p(c)}{q(c)} \\
&= q(c|x) \frac{p(c)}{q(c)}
}

Therefore the correct method of correcting for class redistribution is to multiply probabilities by the ratio $\frac{p(c)}{q(c)}$. Remember to renormalize afterwards.

\subsection{Perspective geometry}

A point in 3-D space can be represented in homogeneous coordinates as $\bmat{ x \\ y \\ z \\ s}$ which is equivalent to the 3D point $\bmat{ x/s \\ y/s \\ z/s }$.

We will now introduce three matrices -- the translation, rotation, and projection matrix.

The translation matrix takes the form

\eq{
	T &= I + \sparen{ \begin{array}{c|c} 0 & \mat{\Delta x \\ \Delta y \\ \Delta z} \\ \hline \mat{\,&0&\,} & 0 \end{array}} \\
\intertext{We can see the effect of the translation matrix on a point}
    T \bmat{x \\ y \\ z \\ s} &= \bmat{x \\ y \\ z \\ s} + s \bmat{\Delta x \\ \Delta y \\ \Delta z \\ 0}
}

Given a 3 by 3 rotation matrix, the homogeneous rotation matrix is simply $\sparen{ \begin{array}{c|c} R & 0 \\ \hline 0 & 1 \end{array}}$
Given angles $\alpha$, $\beta$, and $\gamma$ for rotation about the $x$, $y$, and $z$ axes, the 3 by 3 rotation matrix can be written as a product of rotation matrices:

\eq{
	R &= \bmat{1 & 0 & 0 \\ 0 & \calpha & -\salpha \\ 0 & \salpha & \calpha} 
    \bmat{\cbeta & 0 & \sbeta \\ 0 & 1 & 0 \\ -\sbeta & 0 & \cbeta} 
    \bmat{\cgamma & -\sgamma & 0 \\ \sgamma & \cgamma & 0 \\ 0 & 0 & 1}
}

Note that the rotation and translation matrix can multiplied and combined as
\eq{
	\sparen{ \begin{array}{c|c} R & T \\ \hline 0 & 1 \end{array}}
}

Finally, the projection matrix projects a 3D point onto a 2D camera plane when we have given focal lengths $f_x$ and $f_y$.

\eq{
	K &= \bmat{f_x & 0 & 0 & 0 \\ 0 & f_y & 0 & 0 \\ 0 & 0 & 1 & 0}
}

Finally, we can transform to image coordinates by off-setting the points half of the image width and height and by flipping the $y$ axis:

\eq{
	C &= \bmat{1 & 0 & c_x \\ 0 & -1 & -c_y \\ 0 & 0 & 1}
}

Therefore the full transformation from world coordinates to image points is computed by the transformation:

\eq{
	M &= CKRT
}

\subsubsection{Inverse graphics}

The transformation carried out by the projection matrix is essentially:

\eq{
	x &= \frac{f_x X}{Z}
}

Therefore if we are given the depth of each point on the camera plane, we may invert the process:

\eq{
	X &= \frac{Z}{f_x}x
}

\section{Category Theory}

\subsection{Diagrams}

\begin{tikzcd}[column sep=1.0em, row sep=18.0]
  {}&{}&\Omega
  \ar[rddddd, two heads, "\beta" near start]  
  \ar[rrdd, hook, "\mathcal{C}"]
  &{}&{}
  \\\\
  \Lambda
  \ar[rrrr, two heads, "\delta" near end]
  \ar[uurr, hook, "\mathcal{H}"]  
  &{}&{}&{}&
  \Gamma
  \ar[lllddd, two heads, "\rho" near start]
  \ar[dddl, hook, "\mathcal{G}"]
  \\\\\\
  {}&
  \Xi
  \ar[uuuuur, two heads, "\alpha" near end]
  \ar[luuu, hook, "\mathcal{Z}"]
  &{}&
  \Psi
  \ar[llluuu, two heads, "\chi" near end]  
  \ar[ll, hook, "\mathcal{Q}"]
  &{}
\end{tikzcd}

\subsection{Introduction}

A category is a mathematical object composed of objects and morphisms. Each morphism is an arrow from one object to another. Any two morphisms can be composed to form a new one in the obvious way. There are several requirements that a category must adhere to.

\itmz{
\item The morphisms are closed under composition
\item There is an identity morphism for each object
\item Morphism composition is associative
}

\end{document}
