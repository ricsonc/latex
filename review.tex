\title{Statistics Review}
\author{Ricson}
\date{\today}
\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[expansion=false]{microtype}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{algpseudocode}

\newcommand{\eq}[1]{\begin{align*}#1\end{align*}}
\newcommand{\enum}[1]{\begin{enumerate}#1\end{enumerate}}
\newcommand{\itmz}[1]{\begin{itemize}#1\end{itemize}}
\newcommand{\alg}[1]{\begin{algorithmic}#1\end{algorithmic}}
\newcommand{\ra}{\rightarrow}

\begin{document}
\maketitle

\tableofcontents

\newpage


\section{Probability Theory}

\subsection{Conjugate Priors}

\subsection{Basic manipulations of Multivariate Gaussians}

\subsection{Extremal Value Distributions}

\section{Information Theory}

\subsection{Basics}

\subsection{KL Divergence}

\subsection{Fisher Information Matrix}

\section{Linear Algebra}

\subsection{Identities}

\subsection{Singular Value Decomposition}

\subsection{Principle Component Analysis}

\section{Probabilistic Graphical Models}

\subsection{Forward Backward Algorithm}

\renewcommand{\o}{\mathbf{o}}
\newcommand{\x}{\mathbf{x}}

The key to deriving the forward backward algorithm for the linear-chain CRF is to realize that there is no fundamental difference between HMM and CRF.
The likelihood of a linear-chain CRF is defined to be the product of all the potentials. But before we get mixed up in math here: $o_i$ denotes observation and $x_i$ denotes the hidden state.

\eq{
P(\x) = \prod_i \phi(x_i) \prod_{i,i+1} \phi(x_i, x_{i+1})
}

On the other hand, the likelihood of the equivalent HMM model is the following:

\eq{
P(x) = \prod_i P(o_i | x_i) P(x_i | x_{i+1})
}
Note this product is incorrect when $i = n$, but let's ignore that.

Now if you squint really closely, you're realize that these two equations are actually the same thing. We can just replace $P(o_i|x_i)$ with $\phi(x_i)$ and $P(x_i|x_{i+1})$ with $\phi(x_i, x_{i+1})$. So it suffices to solve inference on the general setting of CRFs. However, we'll start on HMMs simply to avoid a bit of headaches.

We want to solve for $P(x_k|\o)$. The key is the following manipulation:

\eq{
  P(x_k|\o) &= \frac{P(\o|x_k)p(x_k)}{P(\o)} \\
  &\propto P(\o|x_k)p(x_k) \\
  &= P(\o_{1:k}|x_k)P(\o_{k+1:n}|x_k)P(x_k) \\
}

Let's take that first term and apply Bayes' Theorem again

\eq{
  P(\o_{1:k}|x_k) &= \frac{P(x_k | \o_{1:k}) P(\o_{1:k})}{P(x_k)} \\
  &\propto \frac{P(x_k | \o_{1:k})}{P(x_k)}
}

And substitute it back in...

\eq{
  P(\o_{1:k}|x_k)P(\o_{k+1:n}|x_k)P(x_k) &= \frac{P(x_k|\o_{1:k})}{P(x_k)}P(\o_{k+1:n}|x_k)P(x_k) \\
  &= P(x_k|\o_{1:k})P(\o_{k+1:n}|x_k)
}

So now we just need to figure out how to compute $P(x_k|\o_{1:k})$ and $P(o_{k+1:n}|x_k)$. We can do this inductively. To do this, we specify that each $x_i$ is a categorical variables in one of $m$ classes indexed by $j$. In addition, we need $\pi$, the initial distribution of $x_1$ and $T$ being $P(x_{k+1|k})$ and also $Q$ being $P(o_i|x_k)$. 

Our base case $P(x_1|o_1)$ can be solved with Bayes' Theorem

\eq{
  P(x_1|o_1) &= \frac{P(o_1|x_1)P(x_1)}{P(x_1)} \\
  &\propto P(o_1|x_1)P(x_1) \\
  &= Q\pi
}

Then we can simply normalize $Q\pi$ to be a probability vector. Now we will show the inductive step.

\eq{
  P(x_k|\o_{1:k}) &= P(x_k|o_k,\o_{1:k-1}) \\
  &= \frac{P(o_k|x_k,\o_{1:k-1})P(x_k|\o_{1:k-1})}{P(o_k|\o_{1:k-1})} \\
  &\propto P(o_k|x_k,\o_{1:k-1})P(x_k|\o_{1:k-1}) \\
  &= P(o_k|x_k)P(x_k|\o_{1:k-1}) \\
  &= P(o_k|x_k) \sum_j P(x_k|x_{k-1} = j)P(x_{k-1}=j|\o_{1:k-1}) \\
}

All of these quantities are clearly obtainable. We must simply normalize over values of $x_k$ in order to get a proper probability distribution.

In the backward case, we start off with the base case $P(o_n|x_{n-1})$

\eq{
  P(o_n|x_{n-1}) &= \sum_j P(o_n|x_n = j)P(x_n=j|x_{n-1})
}

And for the inductive case we have

\eq{
  P(o_{k+1:n}|x_k) &= \sum_j P(o_{k+1:n}|x_{k+1}=j)P(x_{k+1}=j|x_k) \\
  &= \sum_j P(\o_{k+2:n}|x_{k+1}=j)P(o_{k+1}|x_{k+1} = j)P(x_{k+1}=j|x_k) \\
}

Again, all of these quantities are computable, one via induction, so we are finished here. For CRFs, since factors are not necessarily probabilities, we can carry out an extra normalization step at the end.
\subsection{Message Passing}

\newcommand{\msg}[2]{\mu (#1 \rightarrow #2)}
\renewcommand{\ne}[2]{N(#1) \setminus #2}

\newcommand{\msx}{\mu (s \rightarrow x)}
\newcommand{\mxs}{\mu (x \rightarrow s)}

Some notation:
\itmz{
  \item $x$ is the variable node for which we want the marginal distribution
  \item $\x$ is the total set of variables in the graphic
  \item $f_s$ refers to a factor which is a neighbor of $x$
  \item $x_i'$ refers to the neighbors of $f_s$ excluding $x$.
  \item $f'_k$ refers to a factor which is a neighbor of $x'$
  \item $X_s$ refers to the set of variables $x$ which are below $f_s$
  \item $X_j$ refers to the set of variables $x$ which are below $x_j$ exclusive.
  \item $\msg{s}{x}$ refers to a message from $s$ to $x$.
}

Furthermore, use $F_s(x_j, X_s)$ to denote the product of the factors in the subgraph rooted at $f_s$. Similarly, $G_j(x_j, X_j)$ is used to denote the product of factors in the subgraph rooted at $x_j$. 

We want to solve for the distribution $p(x)$.

\eq{
  p(x) &= \sum_{\x \setminus x} p(\x) \\
  &= \prod_{s \in N(x)} \sum_{X_s} F_s(x, X_s) \\
}

Now note that $\sum_{X_s} F_s(x, X_s)$ is a quantity each factor node $s$ could compute by itself, so it is a good candidate for being a message. Let's say that

\eq{
  \msg{s}{x} &= \sum_{X_s} F_s(x, X_s)
  \intertext{or more generally that}
  \msg{s}{x_i} &= \sum_{X_s} F_s(x_i, X_s)
}

Then we can substitute this back into the equation fork
\eq{
  p(x) &= \prod_{s \in N(x)} \msg{x}{s}
}

It remainds to derive an algorithm for computing $\sum_{X_s} F_s(x, X_s)$ efficiently. First let us derive an expansion for $F_s$.

\eq{
  F_s(x, X_s) &= f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} G_j(x_j, X_j) \\
}

Let $X_s'$ denote $X_s \setminus N(s)$. 

\eq{
  \msg{s}{x} &= \sum_{X_s} f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} G_j(x_j, X_j) \\
  &= \sum_{\ne{s}{x}} \sum_{X_s'} f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} G_j(x_j, X_j) \\
  &= \sum_{\ne{s}{x}} f_s(x, \ne{s}{x}) \sum_{X_s'} \prod_{j \in \ne{s}{x}} G_j(x_j, X_j) \\
  &= \sum_{\ne{s}{x}} f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} \sum_{X_j} G_j(x_j, X_j) \\
}

Just like we noticed that the sum of $F_s$ might be a good candidate for being a message, the sum of $G_j$ is a good candidate, because it can be computed by variable $j$ with no knowledge of anything else.

Let $\msg{j}{s} = \sum_{X_j} G_j(x_j, X_j)$. Then our equation from beforehand becomes

\eq{
  \msg{s}{x} &= \sum_{\ne{s}{x}} f_s(x, \ne{s}{x}) \prod_{j \in \ne{s}{x}} \msg{j}{s} \\
}

So in order for factor $s$ to send variable $x$ the message, it must collect the messages from it's neighbors excepting $x$, take the product, and then marginalize over all variables except $x$.

But what are the messages from it's neighbors? How do we compute $G_j(x_j, X_j)$? Notice that $G_j$ is simply the product of all factors below it, which is the product of the factors in each subtree.

\eq{
  G_j(x_j, X_j) &= \prod_k F_k(x_j, X_k)
}

So now we can compute our message: 

\eq{
  \msg{j}{s} &= \sum_{X_j} G_j(x_j, X_j) \\
  &= \sum_{X_j} \prod_{k \in \ne{j}{s}} F_k(x_j, X_k) \\
  &= \prod_{k \in \ne{j}{s}} \sum_{X_k} F_k(x_j, X_k) \\
  &= \prod_{k \in \ne{j}{s}} \msg{k}{j}
}

So in order for a variable $x_j$ to send factor $f_s$ a message, it must collect the messages from it's child factors, then take the product of them all. Now the whole algorithm is clear. Starting from the leaves, we propagate messages upwards according to these rules until we reach node $x$. At $x$, we simply take the product of all incoming messages and read off the marginal probabilities.

There is just one issue -- the matter of the base cases. When the leaf is a variable, we can see that the product of the incoming messages (there are none) is always 1. Then according to the formula, the value is simply $f(x_l)$.

When the leaf is a factor, the outgoing message is simply the product of all incoming messages, which is always 1. Then the message is also 1.

Another detail is whether there is a more efficient strategy to computing the marginal probabilities of all the nodes (not just one) instead of having to repeat this process once for every single node in the graph, which adds on a factor $n$ complexity.

All that is required to compute the marginal of a given node is that the surrounding factors have sent their message to that factor. Therefore, we can simply run the algorithm above to find the marginal for the root, and then have every variable and factor send messages downwards, starting from the root. This enables every variable node to compute its marginal.

Another small detail is that we actually send $k$ messages at a time, one for each value of each variable $x_i$. 

How can we generalize this algorithm to general graphs, possibly with cycles? Well it turns out that if we initialize all the messages to 1, and then iteratively run this algorithm to simultaneously update all messages, you eventually get the right answer. Maybe. 

\section{Linear and Nonlinear Bayesian Models}

\subsection{Bayesian Linear Regression}

\subsubsection{Evidence Approximation}

\subsection{Bayesian Logistic Regression}

\subsubsection{Laplace Approximation}

\subsection{Spline and Wavelet Bases}

\subsection{Bayesian Neural Networks}
\section{Approximate Inference}

\subsection{Expectation Maximization}

\subsection{Variational Inference}

\subsubsection{Efficient Inference in Fully Connected Gaussian CRFS}

A very popular class of CRFs are dense gaussian crfs, used for image processing. In this case, variables are arranged in an n by n grid, with given unary potentials and pairwise potentials between every pair of points in the grid. 

\section{Sampling Algorithms}

\subsection{Importance Sampling}

\subsection{Monte Carlo Markov Chain}

\subsubsection{Metropolis Hastings}

\subsubsection{Reversible Jump MCMC}

\subsection{Gibbs Sampling}

\section{Optimization}

\subsection{Lagrange Multipliers}

\subsection{Lagrangian Duality}

\subsection{KKT Conditions}

\subsection{SGD Variants}

\subsection{BADMM Algorithm}

\section{Reinforcement Learning}

\subsection{Optimal Control}

\subsubsection{LQR}

\subsubsection{MPC and ILQR}

\subsubsection{Stochastic Optimal Control}

\subsection{Policy Gradients}

\subsection{Natural Policy Gradients}

\subsection{Trust Region Policy Optimization}

\subsection{Guided Policy Search}

\subsection{End to End Deep Visuomotor Policies}

\section{Neural Networks}

\subsection{Variational Autoencoders}

\subsection{Architectures}

\subsubsection{Neural Turing Machine}

\subsubsection{Neural GPU}

\subsubsection{Grid LSTM}

\section{Miscellaneous}

\subsection{Matrix Differentials}

\subsection{Calculus of Variations}

\subsection{Echo State Networks}

\subsection{Gumbel Trick}

\subsection{Support Vector Machines}

\subsection{Gaussian Processes}

\subsection{Independent Component Analysis}

\end{document}
